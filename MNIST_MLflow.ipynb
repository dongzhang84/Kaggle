{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e90f2d2",
   "metadata": {},
   "source": [
    "- MNIST with Tensorflow\n",
    "- MNIST with PyTorch\n",
    "- MNIST with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05661874",
   "metadata": {},
   "source": [
    "# MNIST with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad4b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.6.0\n",
      "Scikit-Learn: 0.24.2\n",
      "Numpy: 1.19.5\n",
      "MLFlow: 1.20.2\n",
      "Matplotlib: 3.4.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten \n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "print(\"TensorFlow: {}\".format(tf.__version__))\n",
    "print(\"Scikit-Learn: {}\".format(sklearn.__version__))\n",
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"MLFlow: {}\".format(mlflow.__version__))\n",
    "print(\"Matplotlib: {}\".format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe680a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1244b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7fb3282f6280>, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap='gray'), print(\"Class: \", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae76e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1],x_train.shape[2], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410b8e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "x_train: (60000, 28, 28, 1)\n",
      "y_train: (60000, 10)\n",
      "x_test: (10000, 28, 28, 1)\n",
      "y_test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes\")\n",
    "print(\"x_train: {}\\ny_train: {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"x_test: {}\\ny_test: {}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796e9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 8)           1160      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                11790     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 13,940\n",
      "Trainable params: 13,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 23:21:33.131885: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=3, strides=2,\n",
    "padding='same', input_shape=(28, 28, 1), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=8, kernel_size=3, strides=2,\n",
    "padding='same', input_shape=(28, 28, 1), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30, activation=\"relu\"))\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce7f5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04835607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'TF_Keras_MNIST' does not exist. Creating a new experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 23:22:41.165777: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-03 23:22:41.165798: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-03 23:22:41.165968: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-10-03 23:22:41.236992: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 17/235 [=>............................] - ETA: 2s - loss: 5.2487 - accuracy: 0.1852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 23:22:41.712095: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-03 23:22:41.712112: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-03 23:22:41.722277: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-10-03 23:22:41.723587: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-10-03 23:22:41.726489: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41\n",
      "\n",
      "2021-10-03 23:22:41.727307: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41/Dongs-MBP.trace.json.gz\n",
      "2021-10-03 23:22:41.730053: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41\n",
      "\n",
      "2021-10-03 23:22:41.730519: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41/Dongs-MBP.memory_profile.json.gz\n",
      "2021-10-03 23:22:41.731570: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41\n",
      "Dumped tool data for xplane.pb to /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41/Dongs-MBP.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41/Dongs-MBP.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41/Dongs-MBP.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41/Dongs-MBP.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmp5srbd9dk/train/plugins/profile/2021_10_03_23_22_41/Dongs-MBP.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 3s 10ms/step - loss: 1.2461 - accuracy: 0.6704\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.2943 - accuracy: 0.9145\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.1821 - accuracy: 0.9459\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.1335 - accuracy: 0.9599\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.1049 - accuracy: 0.9686\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 0.0858 - accuracy: 0.9740\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 3s 11ms/step - loss: 0.0741 - accuracy: 0.9774\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 0.0640 - accuracy: 0.9794\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.0569 - accuracy: 0.9821\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 3s 13ms/step - loss: 0.0509 - accuracy: 0.9838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 23:23:08.049902: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/tmph90a0xfp/model/data/model/assets\n",
      "313/313 [==============================] - 0s 901us/step - loss: 0.0829 - accuracy: 0.9777\n",
      "eval_acc:  0.9776999950408936\n",
      "auc_score:  0.987290041921062\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"TF_Keras_MNIST\") \n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.tensorflow.autolog()\n",
    "    model.fit(x=x_train, y=y_train, batch_size=256, epochs=10)\n",
    "    preds = model.predict(x_test)\n",
    "    preds = np.round(preds)\n",
    "    eval_acc = model.evaluate(x_test, y_test)[1]\n",
    "    auc_score = roc_auc_score(y_test, preds)\n",
    "    print(\"eval_acc: \", eval_acc)\n",
    "    print(\"auc_score: \", auc_score)\n",
    "    mlflow.tensorflow.mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.tensorflow.mlflow.log_metric(\"auc_score\", auc_score)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9193d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.keras.load_model(\"runs:/6bbe24715ff64fc7b9227c86cfdfa15e/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed817f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 927us/step - loss: 0.0829 - accuracy: 0.9777\n",
      "Eval Loss: 0.08294501155614853\n",
      "Eval Acc: 0.9776999950408936\n",
      "Eval AUC: 0.987290041921062\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = loaded_model.evaluate(x_test, y_test)\n",
    "preds = loaded_model.predict(x_test)\n",
    "preds = np.round(preds)\n",
    "eval_auc = roc_auc_score(y_test, preds)\n",
    "print(\"Eval Loss:\", eval_loss)\n",
    "print(\"Eval Acc:\", eval_acc)\n",
    "print(\"Eval AUC:\", eval_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485c090",
   "metadata": {},
   "source": [
    "# MLFlow with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db31dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np \n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c3daaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.9.0\n",
      "torchvision: 0.10.1\n",
      "sklearn: 0.24.2\n",
      "MLFlow: 1.20.2\n",
      "Numpy: 1.19.5\n",
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch: {}\".format(torch.__version__))\n",
    "print(\"torchvision: {}\".format(torchvision.__version__))\n",
    "print(\"sklearn: {}\".format(sklearn.__version__))\n",
    "print(\"MLFlow: {}\".format(mlflow.__version__))\n",
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de985a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "415d83d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c73e5e5c7649739c1fa27719c47566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1122c591f7d2410a8bcfbfbbe0ac3223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a3c71012d94cafa709aff08bdfebf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af3e8e3eba742238042bf113da338b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=None)\n",
    "test_set = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0a435d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_set.data, train_set.targets\n",
    "x_test, y_test = test_set.data, test_set.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c50384a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8cf64b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e90c32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1], x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81d29330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 1, 28, 28]), torch.Size([10000, 1, 28, 28]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24597b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e0e87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(num_classes, labels):\n",
    "    one_hot = torch.zeros(([labels.shape[0], num_classes])) \n",
    "    \n",
    "    for f in range(len(labels)):\n",
    "        one_hot[f][labels[f]] = 1 \n",
    "        \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab48309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_one_hot(num_classes, y_train)\n",
    "y_test = to_one_hot(num_classes, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4cd40ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 10]), torch.Size([10000, 10]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfe983da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "x_train: torch.Size([60000, 1, 28, 28])\n",
      "y_train: torch.Size([60000, 10])\n",
      "x_test: torch.Size([10000, 1, 28, 28])\n",
      "y_test: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes\")\n",
    "print(\"x_train: {}\\ny_train: {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"x_test: {}\\ny_test: {}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6f7f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module): \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        # IN 1x28x28 OUT 16x14x14\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1, dilation=1)\n",
    "        # IN 16x14x14 OUT 32x6x6\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        # IN 32x6x6 OUT 64x2x2\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=0, dilation=1)\n",
    "        # IN 64x2x2 OUT 256\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.dense2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.dense3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.conv1(x) \n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dense2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dense3(x)\n",
    "        x = nn.Softmax()(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9be89605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e742e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(x_train,y_train)\n",
    "train_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf6136f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/ipykernel_49784/2121290085.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = nn.Softmax()(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch_Num 0 Loss 0.3450171649456024\n",
      "Epoch 0 Batch_Num 1 Loss 0.30535024404525757\n",
      "Epoch 0 Batch_Num 2 Loss 0.28882020711898804\n",
      "Epoch 0 Batch_Num 3 Loss 0.27001768350601196\n",
      "Epoch 0 Batch_Num 4 Loss 0.24854028224945068\n",
      "Epoch 0 Batch_Num 5 Loss 0.22579574584960938\n",
      "Epoch 0 Batch_Num 6 Loss 0.1939469277858734\n",
      "Epoch 0 Batch_Num 7 Loss 0.16799435019493103\n",
      "Epoch 0 Batch_Num 8 Loss 0.14004436135292053\n",
      "Epoch 0 Batch_Num 9 Loss 0.14111243188381195\n",
      "Epoch 0 Batch_Num 10 Loss 0.1172834187746048\n",
      "Epoch 0 Batch_Num 11 Loss 0.10994525998830795\n",
      "Epoch 0 Batch_Num 12 Loss 0.10731685161590576\n",
      "Epoch 0 Batch_Num 13 Loss 0.1146308034658432\n",
      "Epoch 0 Batch_Num 14 Loss 0.11636199802160263\n",
      "Epoch 0 Batch_Num 15 Loss 0.09150653332471848\n",
      "Epoch 0 Batch_Num 16 Loss 0.11010614782571793\n",
      "Epoch 0 Batch_Num 17 Loss 0.10004089027643204\n",
      "Epoch 0 Batch_Num 18 Loss 0.10506093502044678\n",
      "Epoch 0 Batch_Num 19 Loss 0.09705551713705063\n",
      "Epoch 0 Batch_Num 20 Loss 0.10725440829992294\n",
      "Epoch 0 Batch_Num 21 Loss 0.06830598413944244\n",
      "Epoch 0 Batch_Num 22 Loss 0.1187787801027298\n",
      "Epoch 0 Batch_Num 23 Loss 0.08304484188556671\n",
      "Epoch 0 Batch_Num 24 Loss 0.0941876769065857\n",
      "Epoch 0 Batch_Num 25 Loss 0.06530755013227463\n",
      "Epoch 0 Batch_Num 26 Loss 0.07947549968957901\n",
      "Epoch 0 Batch_Num 27 Loss 0.08828277140855789\n",
      "Epoch 0 Batch_Num 28 Loss 0.09675048291683197\n",
      "Epoch 0 Batch_Num 29 Loss 0.08440413326025009\n",
      "Epoch 0 Batch_Num 30 Loss 0.09409845620393753\n",
      "Epoch 0 Batch_Num 31 Loss 0.06327009201049805\n",
      "Epoch 0 Batch_Num 32 Loss 0.07878228276968002\n",
      "Epoch 0 Batch_Num 33 Loss 0.07694534957408905\n",
      "Epoch 0 Batch_Num 34 Loss 0.11558768898248672\n",
      "Epoch 0 Batch_Num 35 Loss 0.060236137360334396\n",
      "Epoch 0 Batch_Num 36 Loss 0.08261029422283173\n",
      "Epoch 0 Batch_Num 37 Loss 0.049460653215646744\n",
      "Epoch 0 Batch_Num 38 Loss 0.06396285444498062\n",
      "Epoch 0 Batch_Num 39 Loss 0.07224392890930176\n",
      "Epoch 0 Batch_Num 40 Loss 0.05526398494839668\n",
      "Epoch 0 Batch_Num 41 Loss 0.04486965388059616\n",
      "Epoch 0 Batch_Num 42 Loss 0.04703869670629501\n",
      "Epoch 0 Batch_Num 43 Loss 0.051312416791915894\n",
      "Epoch 0 Batch_Num 44 Loss 0.04477284103631973\n",
      "Epoch 0 Batch_Num 45 Loss 0.10317562520503998\n",
      "Epoch 0 Batch_Num 46 Loss 0.046823032200336456\n",
      "Epoch 0 Batch_Num 47 Loss 0.0547959990799427\n",
      "Epoch 0 Batch_Num 48 Loss 0.07017786800861359\n",
      "Epoch 0 Batch_Num 49 Loss 0.08208303153514862\n",
      "Epoch 0 Batch_Num 50 Loss 0.07283397018909454\n",
      "Epoch 0 Batch_Num 51 Loss 0.07203323394060135\n",
      "Epoch 0 Batch_Num 52 Loss 0.04728490114212036\n",
      "Epoch 0 Batch_Num 53 Loss 0.04691619798541069\n",
      "Epoch 0 Batch_Num 54 Loss 0.07902705669403076\n",
      "Epoch 0 Batch_Num 55 Loss 0.06255318224430084\n",
      "Epoch 0 Batch_Num 56 Loss 0.06882986426353455\n",
      "Epoch 0 Batch_Num 57 Loss 0.07595904916524887\n",
      "Epoch 0 Batch_Num 58 Loss 0.04322902113199234\n",
      "Epoch 0 Batch_Num 59 Loss 0.0446828156709671\n",
      "Epoch 0 Batch_Num 60 Loss 0.04616151005029678\n",
      "Epoch 0 Batch_Num 61 Loss 0.06542100757360458\n",
      "Epoch 0 Batch_Num 62 Loss 0.0714036077260971\n",
      "Epoch 0 Batch_Num 63 Loss 0.03542425110936165\n",
      "Epoch 0 Batch_Num 64 Loss 0.03724478557705879\n",
      "Epoch 0 Batch_Num 65 Loss 0.04811805486679077\n",
      "Epoch 0 Batch_Num 66 Loss 0.05054904893040657\n",
      "Epoch 0 Batch_Num 67 Loss 0.04280094429850578\n",
      "Epoch 0 Batch_Num 68 Loss 0.06084893271327019\n",
      "Epoch 0 Batch_Num 69 Loss 0.04947107657790184\n",
      "Epoch 0 Batch_Num 70 Loss 0.03510687127709389\n",
      "Epoch 0 Batch_Num 71 Loss 0.03321218863129616\n",
      "Epoch 0 Batch_Num 72 Loss 0.032461315393447876\n",
      "Epoch 0 Batch_Num 73 Loss 0.031653109937906265\n",
      "Epoch 0 Batch_Num 74 Loss 0.0353158637881279\n",
      "Epoch 0 Batch_Num 75 Loss 0.040789760649204254\n",
      "Epoch 0 Batch_Num 76 Loss 0.03671032935380936\n",
      "Epoch 0 Batch_Num 77 Loss 0.03672527149319649\n",
      "Epoch 0 Batch_Num 78 Loss 0.05571242421865463\n",
      "Epoch 0 Batch_Num 79 Loss 0.034138381481170654\n",
      "Epoch 0 Batch_Num 80 Loss 0.04844467341899872\n",
      "Epoch 0 Batch_Num 81 Loss 0.05623256042599678\n",
      "Epoch 0 Batch_Num 82 Loss 0.0285093542188406\n",
      "Epoch 0 Batch_Num 83 Loss 0.032863322645425797\n",
      "Epoch 0 Batch_Num 84 Loss 0.041673481464385986\n",
      "Epoch 0 Batch_Num 85 Loss 0.024279924109578133\n",
      "Epoch 0 Batch_Num 86 Loss 0.04313420504331589\n",
      "Epoch 0 Batch_Num 87 Loss 0.04000679776072502\n",
      "Epoch 0 Batch_Num 88 Loss 0.043632738292217255\n",
      "Epoch 0 Batch_Num 89 Loss 0.01927570439875126\n",
      "Epoch 0 Batch_Num 90 Loss 0.022727232426404953\n",
      "Epoch 0 Batch_Num 91 Loss 0.03396681323647499\n",
      "Epoch 0 Batch_Num 92 Loss 0.04478532075881958\n",
      "Epoch 0 Batch_Num 93 Loss 0.029547404497861862\n",
      "Epoch 0 Batch_Num 94 Loss 0.0454489104449749\n",
      "Epoch 0 Batch_Num 95 Loss 0.030425691977143288\n",
      "Epoch 0 Batch_Num 96 Loss 0.05967367812991142\n",
      "Epoch 0 Batch_Num 97 Loss 0.02813595160841942\n",
      "Epoch 0 Batch_Num 98 Loss 0.03295065090060234\n",
      "Epoch 0 Batch_Num 99 Loss 0.022660499438643456\n",
      "Epoch 0 Batch_Num 100 Loss 0.03152469918131828\n",
      "Epoch 0 Batch_Num 101 Loss 0.03165042772889137\n",
      "Epoch 0 Batch_Num 102 Loss 0.020228464156389236\n",
      "Epoch 0 Batch_Num 103 Loss 0.04782947525382042\n",
      "Epoch 0 Batch_Num 104 Loss 0.06120333820581436\n",
      "Epoch 0 Batch_Num 105 Loss 0.029788371175527573\n",
      "Epoch 0 Batch_Num 106 Loss 0.03557281941175461\n",
      "Epoch 0 Batch_Num 107 Loss 0.02934868633747101\n",
      "Epoch 0 Batch_Num 108 Loss 0.039311498403549194\n",
      "Epoch 0 Batch_Num 109 Loss 0.032411910593509674\n",
      "Epoch 0 Batch_Num 110 Loss 0.04586973041296005\n",
      "Epoch 0 Batch_Num 111 Loss 0.03716163709759712\n",
      "Epoch 0 Batch_Num 112 Loss 0.025268394500017166\n",
      "Epoch 0 Batch_Num 113 Loss 0.03705047443509102\n",
      "Epoch 0 Batch_Num 114 Loss 0.03653264790773392\n",
      "Epoch 0 Batch_Num 115 Loss 0.019847074523568153\n",
      "Epoch 0 Batch_Num 116 Loss 0.03883522376418114\n",
      "Epoch 0 Batch_Num 117 Loss 0.03729470074176788\n",
      "Epoch 0 Batch_Num 118 Loss 0.022647656500339508\n",
      "Epoch 0 Batch_Num 119 Loss 0.035977594554424286\n",
      "Epoch 0 Batch_Num 120 Loss 0.0364806093275547\n",
      "Epoch 0 Batch_Num 121 Loss 0.021258780732750893\n",
      "Epoch 0 Batch_Num 122 Loss 0.0548265166580677\n",
      "Epoch 0 Batch_Num 123 Loss 0.04053177312016487\n",
      "Epoch 0 Batch_Num 124 Loss 0.0390792116522789\n",
      "Epoch 0 Batch_Num 125 Loss 0.038533926010131836\n",
      "Epoch 0 Batch_Num 126 Loss 0.04758226126432419\n",
      "Epoch 0 Batch_Num 127 Loss 0.016764726489782333\n",
      "Epoch 0 Batch_Num 128 Loss 0.022737588733434677\n",
      "Epoch 0 Batch_Num 129 Loss 0.02303759753704071\n",
      "Epoch 0 Batch_Num 130 Loss 0.0336415097117424\n",
      "Epoch 0 Batch_Num 131 Loss 0.028493136167526245\n",
      "Epoch 0 Batch_Num 132 Loss 0.024066176265478134\n",
      "Epoch 0 Batch_Num 133 Loss 0.021792273968458176\n",
      "Epoch 0 Batch_Num 134 Loss 0.03284472972154617\n",
      "Epoch 0 Batch_Num 135 Loss 0.035002417862415314\n",
      "Epoch 0 Batch_Num 136 Loss 0.024624817073345184\n",
      "Epoch 0 Batch_Num 137 Loss 0.03091059997677803\n",
      "Epoch 0 Batch_Num 138 Loss 0.021838366985321045\n",
      "Epoch 0 Batch_Num 139 Loss 0.015599137172102928\n",
      "Epoch 0 Batch_Num 140 Loss 0.02641589567065239\n",
      "Epoch 0 Batch_Num 141 Loss 0.026269715279340744\n",
      "Epoch 0 Batch_Num 142 Loss 0.03293924406170845\n",
      "Epoch 0 Batch_Num 143 Loss 0.018594816327095032\n",
      "Epoch 0 Batch_Num 144 Loss 0.019384309649467468\n",
      "Epoch 0 Batch_Num 145 Loss 0.028270095586776733\n",
      "Epoch 0 Batch_Num 146 Loss 0.050025008618831635\n",
      "Epoch 0 Batch_Num 147 Loss 0.03812145069241524\n",
      "Epoch 0 Batch_Num 148 Loss 0.018957603722810745\n",
      "Epoch 0 Batch_Num 149 Loss 0.026434043422341347\n",
      "Epoch 0 Batch_Num 150 Loss 0.03126855194568634\n",
      "Epoch 0 Batch_Num 151 Loss 0.025602951645851135\n",
      "Epoch 0 Batch_Num 152 Loss 0.019539814442396164\n",
      "Epoch 0 Batch_Num 153 Loss 0.05009273439645767\n",
      "Epoch 0 Batch_Num 154 Loss 0.03228871524333954\n",
      "Epoch 0 Batch_Num 155 Loss 0.026446323841810226\n",
      "Epoch 0 Batch_Num 156 Loss 0.013938521035015583\n",
      "Epoch 0 Batch_Num 157 Loss 0.019298218190670013\n",
      "Epoch 0 Batch_Num 158 Loss 0.023592593148350716\n",
      "Epoch 0 Batch_Num 159 Loss 0.016604645177721977\n",
      "Epoch 0 Batch_Num 160 Loss 0.02683473564684391\n",
      "Epoch 0 Batch_Num 161 Loss 0.038180671632289886\n",
      "Epoch 0 Batch_Num 162 Loss 0.022825779393315315\n",
      "Epoch 0 Batch_Num 163 Loss 0.026904284954071045\n",
      "Epoch 0 Batch_Num 164 Loss 0.02927873097360134\n",
      "Epoch 0 Batch_Num 165 Loss 0.02924487553536892\n",
      "Epoch 0 Batch_Num 166 Loss 0.02914864756166935\n",
      "Epoch 0 Batch_Num 167 Loss 0.030720483511686325\n",
      "Epoch 0 Batch_Num 168 Loss 0.02907833829522133\n",
      "Epoch 0 Batch_Num 169 Loss 0.013040408492088318\n",
      "Epoch 0 Batch_Num 170 Loss 0.01782187446951866\n",
      "Epoch 0 Batch_Num 171 Loss 0.03155350685119629\n",
      "Epoch 0 Batch_Num 172 Loss 0.03524041175842285\n",
      "Epoch 0 Batch_Num 173 Loss 0.026383444666862488\n",
      "Epoch 0 Batch_Num 174 Loss 0.010663958266377449\n",
      "Epoch 0 Batch_Num 175 Loss 0.031218301504850388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch_Num 176 Loss 0.015677234157919884\n",
      "Epoch 0 Batch_Num 177 Loss 0.020107442513108253\n",
      "Epoch 0 Batch_Num 178 Loss 0.028509825468063354\n",
      "Epoch 0 Batch_Num 179 Loss 0.0254000723361969\n",
      "Epoch 0 Batch_Num 180 Loss 0.0465630404651165\n",
      "Epoch 0 Batch_Num 181 Loss 0.02171119675040245\n",
      "Epoch 0 Batch_Num 182 Loss 0.022864174097776413\n",
      "Epoch 0 Batch_Num 183 Loss 0.02781274914741516\n",
      "Epoch 0 Batch_Num 184 Loss 0.03190147131681442\n",
      "Epoch 0 Batch_Num 185 Loss 0.026436885818839073\n",
      "Epoch 0 Batch_Num 186 Loss 0.019762489944696426\n",
      "Epoch 0 Batch_Num 187 Loss 0.021450847387313843\n",
      "Epoch 0 Batch_Num 188 Loss 0.014787012711167336\n",
      "Epoch 0 Batch_Num 189 Loss 0.015613983385264874\n",
      "Epoch 0 Batch_Num 190 Loss 0.0132725415751338\n",
      "Epoch 0 Batch_Num 191 Loss 0.04278307780623436\n",
      "Epoch 0 Batch_Num 192 Loss 0.022091522812843323\n",
      "Epoch 0 Batch_Num 193 Loss 0.03883367404341698\n",
      "Epoch 0 Batch_Num 194 Loss 0.02798197790980339\n",
      "Epoch 0 Batch_Num 195 Loss 0.020014073699712753\n",
      "Epoch 0 Batch_Num 196 Loss 0.03847440704703331\n",
      "Epoch 0 Batch_Num 197 Loss 0.022902686148881912\n",
      "Epoch 0 Batch_Num 198 Loss 0.021958626806735992\n",
      "Epoch 0 Batch_Num 199 Loss 0.011678698472678661\n",
      "Epoch 0 Batch_Num 200 Loss 0.02025732584297657\n",
      "Epoch 0 Batch_Num 201 Loss 0.021650394424796104\n",
      "Epoch 0 Batch_Num 202 Loss 0.014669214375317097\n",
      "Epoch 0 Batch_Num 203 Loss 0.037498004734516144\n",
      "Epoch 0 Batch_Num 204 Loss 0.020604748278856277\n",
      "Epoch 0 Batch_Num 205 Loss 0.01834203116595745\n",
      "Epoch 0 Batch_Num 206 Loss 0.04585598409175873\n",
      "Epoch 0 Batch_Num 207 Loss 0.01889919675886631\n",
      "Epoch 0 Batch_Num 208 Loss 0.010945567861199379\n",
      "Epoch 0 Batch_Num 209 Loss 0.02562827244400978\n",
      "Epoch 0 Batch_Num 210 Loss 0.04479695111513138\n",
      "Epoch 0 Batch_Num 211 Loss 0.021194593980908394\n",
      "Epoch 0 Batch_Num 212 Loss 0.014944116584956646\n",
      "Epoch 0 Batch_Num 213 Loss 0.013442037627100945\n",
      "Epoch 0 Batch_Num 214 Loss 0.03194614499807358\n",
      "Epoch 0 Batch_Num 215 Loss 0.01873696967959404\n",
      "Epoch 0 Batch_Num 216 Loss 0.028503119945526123\n",
      "Epoch 0 Batch_Num 217 Loss 0.023592542856931686\n",
      "Epoch 0 Batch_Num 218 Loss 0.030093956738710403\n",
      "Epoch 0 Batch_Num 219 Loss 0.022766046226024628\n",
      "Epoch 0 Batch_Num 220 Loss 0.020467611029744148\n",
      "Epoch 0 Batch_Num 221 Loss 0.028200283646583557\n",
      "Epoch 0 Batch_Num 222 Loss 0.011914856731891632\n",
      "Epoch 0 Batch_Num 223 Loss 0.0149723831564188\n",
      "Epoch 0 Batch_Num 224 Loss 0.014940393157303333\n",
      "Epoch 0 Batch_Num 225 Loss 0.02238713577389717\n",
      "Epoch 0 Batch_Num 226 Loss 0.013129589147865772\n",
      "Epoch 0 Batch_Num 227 Loss 0.0035486281849443913\n",
      "Epoch 0 Batch_Num 228 Loss 0.011177947744727135\n",
      "Epoch 0 Batch_Num 229 Loss 0.029547512531280518\n",
      "Epoch 0 Batch_Num 230 Loss 0.0031738928519189358\n",
      "Epoch 0 Batch_Num 231 Loss 0.010696071200072765\n",
      "Epoch 0 Batch_Num 232 Loss 0.005674122832715511\n",
      "Epoch 0 Batch_Num 233 Loss 0.02941354736685753\n",
      "Epoch 0 Batch_Num 234 Loss 0.03503769636154175\n",
      "Epoch 1 Batch_Num 0 Loss 0.025355923920869827\n",
      "Epoch 1 Batch_Num 1 Loss 0.019338399171829224\n",
      "Epoch 1 Batch_Num 2 Loss 0.01363815926015377\n",
      "Epoch 1 Batch_Num 3 Loss 0.03850236162543297\n",
      "Epoch 1 Batch_Num 4 Loss 0.03003392554819584\n",
      "Epoch 1 Batch_Num 5 Loss 0.0281564649194479\n",
      "Epoch 1 Batch_Num 6 Loss 0.016011890023946762\n",
      "Epoch 1 Batch_Num 7 Loss 0.016288045793771744\n",
      "Epoch 1 Batch_Num 8 Loss 0.019238118082284927\n",
      "Epoch 1 Batch_Num 9 Loss 0.01056710071861744\n",
      "Epoch 1 Batch_Num 10 Loss 0.029653239995241165\n",
      "Epoch 1 Batch_Num 11 Loss 0.023613644763827324\n",
      "Epoch 1 Batch_Num 12 Loss 0.015007412061095238\n",
      "Epoch 1 Batch_Num 13 Loss 0.02223154529929161\n",
      "Epoch 1 Batch_Num 14 Loss 0.023554036393761635\n",
      "Epoch 1 Batch_Num 15 Loss 0.020115908235311508\n",
      "Epoch 1 Batch_Num 16 Loss 0.012386789545416832\n",
      "Epoch 1 Batch_Num 17 Loss 0.01343205850571394\n",
      "Epoch 1 Batch_Num 18 Loss 0.017096247524023056\n",
      "Epoch 1 Batch_Num 19 Loss 0.022651273757219315\n",
      "Epoch 1 Batch_Num 20 Loss 0.02825072966516018\n",
      "Epoch 1 Batch_Num 21 Loss 0.01328725554049015\n",
      "Epoch 1 Batch_Num 22 Loss 0.029644107446074486\n",
      "Epoch 1 Batch_Num 23 Loss 0.013653496280312538\n",
      "Epoch 1 Batch_Num 24 Loss 0.018398936837911606\n",
      "Epoch 1 Batch_Num 25 Loss 0.01455360371619463\n",
      "Epoch 1 Batch_Num 26 Loss 0.0223481897264719\n",
      "Epoch 1 Batch_Num 27 Loss 0.01703723333775997\n",
      "Epoch 1 Batch_Num 28 Loss 0.023213200271129608\n",
      "Epoch 1 Batch_Num 29 Loss 0.016463592648506165\n",
      "Epoch 1 Batch_Num 30 Loss 0.028556371107697487\n",
      "Epoch 1 Batch_Num 31 Loss 0.01406231801956892\n",
      "Epoch 1 Batch_Num 32 Loss 0.0244494266808033\n",
      "Epoch 1 Batch_Num 33 Loss 0.02322022244334221\n",
      "Epoch 1 Batch_Num 34 Loss 0.04028918594121933\n",
      "Epoch 1 Batch_Num 35 Loss 0.012959837913513184\n",
      "Epoch 1 Batch_Num 36 Loss 0.0227850079536438\n",
      "Epoch 1 Batch_Num 37 Loss 0.014250087551772594\n",
      "Epoch 1 Batch_Num 38 Loss 0.010162431746721268\n",
      "Epoch 1 Batch_Num 39 Loss 0.02380615845322609\n",
      "Epoch 1 Batch_Num 40 Loss 0.01650148443877697\n",
      "Epoch 1 Batch_Num 41 Loss 0.011721577495336533\n",
      "Epoch 1 Batch_Num 42 Loss 0.017517216503620148\n",
      "Epoch 1 Batch_Num 43 Loss 0.013815587386488914\n",
      "Epoch 1 Batch_Num 44 Loss 0.010251877829432487\n",
      "Epoch 1 Batch_Num 45 Loss 0.030248474329710007\n",
      "Epoch 1 Batch_Num 46 Loss 0.018921852111816406\n",
      "Epoch 1 Batch_Num 47 Loss 0.01098750252276659\n",
      "Epoch 1 Batch_Num 48 Loss 0.015556780621409416\n",
      "Epoch 1 Batch_Num 49 Loss 0.024144750088453293\n",
      "Epoch 1 Batch_Num 50 Loss 0.01999969407916069\n",
      "Epoch 1 Batch_Num 51 Loss 0.024133529514074326\n",
      "Epoch 1 Batch_Num 52 Loss 0.012973019853234291\n",
      "Epoch 1 Batch_Num 53 Loss 0.014296273700892925\n",
      "Epoch 1 Batch_Num 54 Loss 0.019185371696949005\n",
      "Epoch 1 Batch_Num 55 Loss 0.02324301190674305\n",
      "Epoch 1 Batch_Num 56 Loss 0.019229266792535782\n",
      "Epoch 1 Batch_Num 57 Loss 0.023822881281375885\n",
      "Epoch 1 Batch_Num 58 Loss 0.009071770124137402\n",
      "Epoch 1 Batch_Num 59 Loss 0.020871464163064957\n",
      "Epoch 1 Batch_Num 60 Loss 0.015305948443710804\n",
      "Epoch 1 Batch_Num 61 Loss 0.022228220477700233\n",
      "Epoch 1 Batch_Num 62 Loss 0.02201000414788723\n",
      "Epoch 1 Batch_Num 63 Loss 0.014146005734801292\n",
      "Epoch 1 Batch_Num 64 Loss 0.014432862401008606\n",
      "Epoch 1 Batch_Num 65 Loss 0.017482664436101913\n",
      "Epoch 1 Batch_Num 66 Loss 0.01873190701007843\n",
      "Epoch 1 Batch_Num 67 Loss 0.01694496162235737\n",
      "Epoch 1 Batch_Num 68 Loss 0.02303350158035755\n",
      "Epoch 1 Batch_Num 69 Loss 0.02217869646847248\n",
      "Epoch 1 Batch_Num 70 Loss 0.01436267513781786\n",
      "Epoch 1 Batch_Num 71 Loss 0.01638294756412506\n",
      "Epoch 1 Batch_Num 72 Loss 0.01346192043274641\n",
      "Epoch 1 Batch_Num 73 Loss 0.010825353674590588\n",
      "Epoch 1 Batch_Num 74 Loss 0.0196889266371727\n",
      "Epoch 1 Batch_Num 75 Loss 0.018348297104239464\n",
      "Epoch 1 Batch_Num 76 Loss 0.015621108002960682\n",
      "Epoch 1 Batch_Num 77 Loss 0.014441234990954399\n",
      "Epoch 1 Batch_Num 78 Loss 0.029076049104332924\n",
      "Epoch 1 Batch_Num 79 Loss 0.01385872345417738\n",
      "Epoch 1 Batch_Num 80 Loss 0.013132224790751934\n",
      "Epoch 1 Batch_Num 81 Loss 0.02074688859283924\n",
      "Epoch 1 Batch_Num 82 Loss 0.011777833104133606\n",
      "Epoch 1 Batch_Num 83 Loss 0.014956055209040642\n",
      "Epoch 1 Batch_Num 84 Loss 0.01107779610902071\n",
      "Epoch 1 Batch_Num 85 Loss 0.007076936308294535\n",
      "Epoch 1 Batch_Num 86 Loss 0.012622592970728874\n",
      "Epoch 1 Batch_Num 87 Loss 0.015873197466135025\n",
      "Epoch 1 Batch_Num 88 Loss 0.02138454094529152\n",
      "Epoch 1 Batch_Num 89 Loss 0.008554606698453426\n",
      "Epoch 1 Batch_Num 90 Loss 0.014652617275714874\n",
      "Epoch 1 Batch_Num 91 Loss 0.010430620983242989\n",
      "Epoch 1 Batch_Num 92 Loss 0.01729651540517807\n",
      "Epoch 1 Batch_Num 93 Loss 0.017109563574194908\n",
      "Epoch 1 Batch_Num 94 Loss 0.02248014137148857\n",
      "Epoch 1 Batch_Num 95 Loss 0.008367318660020828\n",
      "Epoch 1 Batch_Num 96 Loss 0.03354822099208832\n",
      "Epoch 1 Batch_Num 97 Loss 0.010839652270078659\n",
      "Epoch 1 Batch_Num 98 Loss 0.015005761757493019\n",
      "Epoch 1 Batch_Num 99 Loss 0.0108665581792593\n",
      "Epoch 1 Batch_Num 100 Loss 0.013057204894721508\n",
      "Epoch 1 Batch_Num 101 Loss 0.01579272747039795\n",
      "Epoch 1 Batch_Num 102 Loss 0.008185086771845818\n",
      "Epoch 1 Batch_Num 103 Loss 0.037846971303224564\n",
      "Epoch 1 Batch_Num 104 Loss 0.03777460381388664\n",
      "Epoch 1 Batch_Num 105 Loss 0.018055269494652748\n",
      "Epoch 1 Batch_Num 106 Loss 0.014519807882606983\n",
      "Epoch 1 Batch_Num 107 Loss 0.0172277744859457\n",
      "Epoch 1 Batch_Num 108 Loss 0.022953703999519348\n",
      "Epoch 1 Batch_Num 109 Loss 0.010859420523047447\n",
      "Epoch 1 Batch_Num 110 Loss 0.01510109007358551\n",
      "Epoch 1 Batch_Num 111 Loss 0.016865072771906853\n",
      "Epoch 1 Batch_Num 112 Loss 0.012713382951915264\n",
      "Epoch 1 Batch_Num 113 Loss 0.018383637070655823\n",
      "Epoch 1 Batch_Num 114 Loss 0.014353690668940544\n",
      "Epoch 1 Batch_Num 115 Loss 0.014237634837627411\n",
      "Epoch 1 Batch_Num 116 Loss 0.021952489390969276\n",
      "Epoch 1 Batch_Num 117 Loss 0.023814499378204346\n",
      "Epoch 1 Batch_Num 118 Loss 0.006912699900567532\n",
      "Epoch 1 Batch_Num 119 Loss 0.01774999126791954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch_Num 120 Loss 0.016568202525377274\n",
      "Epoch 1 Batch_Num 121 Loss 0.01573582924902439\n",
      "Epoch 1 Batch_Num 122 Loss 0.01425081305205822\n",
      "Epoch 1 Batch_Num 123 Loss 0.021069318056106567\n",
      "Epoch 1 Batch_Num 124 Loss 0.019147804006934166\n",
      "Epoch 1 Batch_Num 125 Loss 0.01673278585076332\n",
      "Epoch 1 Batch_Num 126 Loss 0.025724321603775024\n",
      "Epoch 1 Batch_Num 127 Loss 0.004336982499808073\n",
      "Epoch 1 Batch_Num 128 Loss 0.01439958531409502\n",
      "Epoch 1 Batch_Num 129 Loss 0.013230107724666595\n",
      "Epoch 1 Batch_Num 130 Loss 0.021241944283246994\n",
      "Epoch 1 Batch_Num 131 Loss 0.013282785192131996\n",
      "Epoch 1 Batch_Num 132 Loss 0.008129538036882877\n",
      "Epoch 1 Batch_Num 133 Loss 0.006858639419078827\n",
      "Epoch 1 Batch_Num 134 Loss 0.024773910641670227\n",
      "Epoch 1 Batch_Num 135 Loss 0.024461552500724792\n",
      "Epoch 1 Batch_Num 136 Loss 0.014338038861751556\n",
      "Epoch 1 Batch_Num 137 Loss 0.01681116409599781\n",
      "Epoch 1 Batch_Num 138 Loss 0.012148609384894371\n",
      "Epoch 1 Batch_Num 139 Loss 0.008343199267983437\n",
      "Epoch 1 Batch_Num 140 Loss 0.020105933770537376\n",
      "Epoch 1 Batch_Num 141 Loss 0.01806044951081276\n",
      "Epoch 1 Batch_Num 142 Loss 0.019833417609333992\n",
      "Epoch 1 Batch_Num 143 Loss 0.0199658814817667\n",
      "Epoch 1 Batch_Num 144 Loss 0.015590101480484009\n",
      "Epoch 1 Batch_Num 145 Loss 0.012345361523330212\n",
      "Epoch 1 Batch_Num 146 Loss 0.02529638074338436\n",
      "Epoch 1 Batch_Num 147 Loss 0.030379047617316246\n",
      "Epoch 1 Batch_Num 148 Loss 0.010634621605277061\n",
      "Epoch 1 Batch_Num 149 Loss 0.012689243070781231\n",
      "Epoch 1 Batch_Num 150 Loss 0.01489689014852047\n",
      "Epoch 1 Batch_Num 151 Loss 0.010169925168156624\n",
      "Epoch 1 Batch_Num 152 Loss 0.006729742046445608\n",
      "Epoch 1 Batch_Num 153 Loss 0.03254033997654915\n",
      "Epoch 1 Batch_Num 154 Loss 0.018412034958600998\n",
      "Epoch 1 Batch_Num 155 Loss 0.01213769055902958\n",
      "Epoch 1 Batch_Num 156 Loss 0.01107881497591734\n",
      "Epoch 1 Batch_Num 157 Loss 0.014036484062671661\n",
      "Epoch 1 Batch_Num 158 Loss 0.011735869571566582\n",
      "Epoch 1 Batch_Num 159 Loss 0.014492414891719818\n",
      "Epoch 1 Batch_Num 160 Loss 0.0075706304050982\n",
      "Epoch 1 Batch_Num 161 Loss 0.027338311076164246\n",
      "Epoch 1 Batch_Num 162 Loss 0.01196015439927578\n",
      "Epoch 1 Batch_Num 163 Loss 0.015085023827850819\n",
      "Epoch 1 Batch_Num 164 Loss 0.014172246679663658\n",
      "Epoch 1 Batch_Num 165 Loss 0.011219930835068226\n",
      "Epoch 1 Batch_Num 166 Loss 0.01848384365439415\n",
      "Epoch 1 Batch_Num 167 Loss 0.017893314361572266\n",
      "Epoch 1 Batch_Num 168 Loss 0.016896283254027367\n",
      "Epoch 1 Batch_Num 169 Loss 0.00921234767884016\n",
      "Epoch 1 Batch_Num 170 Loss 0.015848975628614426\n",
      "Epoch 1 Batch_Num 171 Loss 0.012998747639358044\n",
      "Epoch 1 Batch_Num 172 Loss 0.013793379068374634\n",
      "Epoch 1 Batch_Num 173 Loss 0.017474455758929253\n",
      "Epoch 1 Batch_Num 174 Loss 0.008447468280792236\n",
      "Epoch 1 Batch_Num 175 Loss 0.013201117515563965\n",
      "Epoch 1 Batch_Num 176 Loss 0.009363843128085136\n",
      "Epoch 1 Batch_Num 177 Loss 0.014732496812939644\n",
      "Epoch 1 Batch_Num 178 Loss 0.01614725962281227\n",
      "Epoch 1 Batch_Num 179 Loss 0.016261888667941093\n",
      "Epoch 1 Batch_Num 180 Loss 0.025338808074593544\n",
      "Epoch 1 Batch_Num 181 Loss 0.014243978075683117\n",
      "Epoch 1 Batch_Num 182 Loss 0.013419287279248238\n",
      "Epoch 1 Batch_Num 183 Loss 0.01590694859623909\n",
      "Epoch 1 Batch_Num 184 Loss 0.022600801661610603\n",
      "Epoch 1 Batch_Num 185 Loss 0.016348693519830704\n",
      "Epoch 1 Batch_Num 186 Loss 0.010811211541295052\n",
      "Epoch 1 Batch_Num 187 Loss 0.013400794938206673\n",
      "Epoch 1 Batch_Num 188 Loss 0.009507106617093086\n",
      "Epoch 1 Batch_Num 189 Loss 0.005145718343555927\n",
      "Epoch 1 Batch_Num 190 Loss 0.004375707358121872\n",
      "Epoch 1 Batch_Num 191 Loss 0.021125327795743942\n",
      "Epoch 1 Batch_Num 192 Loss 0.011968210339546204\n",
      "Epoch 1 Batch_Num 193 Loss 0.02211793139576912\n",
      "Epoch 1 Batch_Num 194 Loss 0.009885070845484734\n",
      "Epoch 1 Batch_Num 195 Loss 0.007326266262680292\n",
      "Epoch 1 Batch_Num 196 Loss 0.023335378617048264\n",
      "Epoch 1 Batch_Num 197 Loss 0.009267304092645645\n",
      "Epoch 1 Batch_Num 198 Loss 0.012297071516513824\n",
      "Epoch 1 Batch_Num 199 Loss 0.007996594533324242\n",
      "Epoch 1 Batch_Num 200 Loss 0.009074846282601357\n",
      "Epoch 1 Batch_Num 201 Loss 0.012310290709137917\n",
      "Epoch 1 Batch_Num 202 Loss 0.012385557405650616\n",
      "Epoch 1 Batch_Num 203 Loss 0.016695108264684677\n",
      "Epoch 1 Batch_Num 204 Loss 0.016160963103175163\n",
      "Epoch 1 Batch_Num 205 Loss 0.007813982665538788\n",
      "Epoch 1 Batch_Num 206 Loss 0.027619723230600357\n",
      "Epoch 1 Batch_Num 207 Loss 0.012335965409874916\n",
      "Epoch 1 Batch_Num 208 Loss 0.010883746668696404\n",
      "Epoch 1 Batch_Num 209 Loss 0.014624561183154583\n",
      "Epoch 1 Batch_Num 210 Loss 0.017742745578289032\n",
      "Epoch 1 Batch_Num 211 Loss 0.012789052911102772\n",
      "Epoch 1 Batch_Num 212 Loss 0.011055946350097656\n",
      "Epoch 1 Batch_Num 213 Loss 0.008553813211619854\n",
      "Epoch 1 Batch_Num 214 Loss 0.014342772774398327\n",
      "Epoch 1 Batch_Num 215 Loss 0.009878121316432953\n",
      "Epoch 1 Batch_Num 216 Loss 0.016832588240504265\n",
      "Epoch 1 Batch_Num 217 Loss 0.012727966532111168\n",
      "Epoch 1 Batch_Num 218 Loss 0.012762161903083324\n",
      "Epoch 1 Batch_Num 219 Loss 0.014282384887337685\n",
      "Epoch 1 Batch_Num 220 Loss 0.013504239730536938\n",
      "Epoch 1 Batch_Num 221 Loss 0.013801032677292824\n",
      "Epoch 1 Batch_Num 222 Loss 0.006484177894890308\n",
      "Epoch 1 Batch_Num 223 Loss 0.009946227073669434\n",
      "Epoch 1 Batch_Num 224 Loss 0.007399926893413067\n",
      "Epoch 1 Batch_Num 225 Loss 0.01705285534262657\n",
      "Epoch 1 Batch_Num 226 Loss 0.010290546342730522\n",
      "Epoch 1 Batch_Num 227 Loss 0.0017134000081568956\n",
      "Epoch 1 Batch_Num 228 Loss 0.004055870231240988\n",
      "Epoch 1 Batch_Num 229 Loss 0.007677995599806309\n",
      "Epoch 1 Batch_Num 230 Loss 0.0009124054340645671\n",
      "Epoch 1 Batch_Num 231 Loss 0.004241724964231253\n",
      "Epoch 1 Batch_Num 232 Loss 0.004730360582470894\n",
      "Epoch 1 Batch_Num 233 Loss 0.016281161457300186\n",
      "Epoch 1 Batch_Num 234 Loss 0.026347573846578598\n",
      "Epoch 2 Batch_Num 0 Loss 0.0185441542416811\n",
      "Epoch 2 Batch_Num 1 Loss 0.019798550754785538\n",
      "Epoch 2 Batch_Num 2 Loss 0.008812720887362957\n",
      "Epoch 2 Batch_Num 3 Loss 0.01973040960729122\n",
      "Epoch 2 Batch_Num 4 Loss 0.014666521921753883\n",
      "Epoch 2 Batch_Num 5 Loss 0.015698635950684547\n",
      "Epoch 2 Batch_Num 6 Loss 0.009482132270932198\n",
      "Epoch 2 Batch_Num 7 Loss 0.010911998338997364\n",
      "Epoch 2 Batch_Num 8 Loss 0.012172003276646137\n",
      "Epoch 2 Batch_Num 9 Loss 0.004551635589450598\n",
      "Epoch 2 Batch_Num 10 Loss 0.01830410771071911\n",
      "Epoch 2 Batch_Num 11 Loss 0.012422187253832817\n",
      "Epoch 2 Batch_Num 12 Loss 0.004480942152440548\n",
      "Epoch 2 Batch_Num 13 Loss 0.013162851333618164\n",
      "Epoch 2 Batch_Num 14 Loss 0.01394260860979557\n",
      "Epoch 2 Batch_Num 15 Loss 0.008858650922775269\n",
      "Epoch 2 Batch_Num 16 Loss 0.007898963987827301\n",
      "Epoch 2 Batch_Num 17 Loss 0.008646207861602306\n",
      "Epoch 2 Batch_Num 18 Loss 0.008335714228451252\n",
      "Epoch 2 Batch_Num 19 Loss 0.019229304045438766\n",
      "Epoch 2 Batch_Num 20 Loss 0.01552000641822815\n",
      "Epoch 2 Batch_Num 21 Loss 0.00512893358245492\n",
      "Epoch 2 Batch_Num 22 Loss 0.01787758246064186\n",
      "Epoch 2 Batch_Num 23 Loss 0.011968890205025673\n",
      "Epoch 2 Batch_Num 24 Loss 0.019919048994779587\n",
      "Epoch 2 Batch_Num 25 Loss 0.012412475422024727\n",
      "Epoch 2 Batch_Num 26 Loss 0.01828000880777836\n",
      "Epoch 2 Batch_Num 27 Loss 0.013415801338851452\n",
      "Epoch 2 Batch_Num 28 Loss 0.013209961354732513\n",
      "Epoch 2 Batch_Num 29 Loss 0.010706466622650623\n",
      "Epoch 2 Batch_Num 30 Loss 0.012294115498661995\n",
      "Epoch 2 Batch_Num 31 Loss 0.007605587597936392\n",
      "Epoch 2 Batch_Num 32 Loss 0.019298432394862175\n",
      "Epoch 2 Batch_Num 33 Loss 0.012995630502700806\n",
      "Epoch 2 Batch_Num 34 Loss 0.023129595443606377\n",
      "Epoch 2 Batch_Num 35 Loss 0.00827980600297451\n",
      "Epoch 2 Batch_Num 36 Loss 0.01633988507091999\n",
      "Epoch 2 Batch_Num 37 Loss 0.00812745001167059\n",
      "Epoch 2 Batch_Num 38 Loss 0.007007950451225042\n",
      "Epoch 2 Batch_Num 39 Loss 0.016669774428009987\n",
      "Epoch 2 Batch_Num 40 Loss 0.017786046490073204\n",
      "Epoch 2 Batch_Num 41 Loss 0.006500582210719585\n",
      "Epoch 2 Batch_Num 42 Loss 0.013470468111336231\n",
      "Epoch 2 Batch_Num 43 Loss 0.0071611651219427586\n",
      "Epoch 2 Batch_Num 44 Loss 0.0059276483952999115\n",
      "Epoch 2 Batch_Num 45 Loss 0.01963024027645588\n",
      "Epoch 2 Batch_Num 46 Loss 0.024302849546074867\n",
      "Epoch 2 Batch_Num 47 Loss 0.009702585637569427\n",
      "Epoch 2 Batch_Num 48 Loss 0.010340849868953228\n",
      "Epoch 2 Batch_Num 49 Loss 0.01725662313401699\n",
      "Epoch 2 Batch_Num 50 Loss 0.010274678468704224\n",
      "Epoch 2 Batch_Num 51 Loss 0.0175340473651886\n",
      "Epoch 2 Batch_Num 52 Loss 0.0068779028952121735\n",
      "Epoch 2 Batch_Num 53 Loss 0.012263809330761433\n",
      "Epoch 2 Batch_Num 54 Loss 0.011491269804537296\n",
      "Epoch 2 Batch_Num 55 Loss 0.015128513798117638\n",
      "Epoch 2 Batch_Num 56 Loss 0.014653538353741169\n",
      "Epoch 2 Batch_Num 57 Loss 0.015988649800419807\n",
      "Epoch 2 Batch_Num 58 Loss 0.00553469592705369\n",
      "Epoch 2 Batch_Num 59 Loss 0.012214619666337967\n",
      "Epoch 2 Batch_Num 60 Loss 0.008871962316334248\n",
      "Epoch 2 Batch_Num 61 Loss 0.015661953017115593\n",
      "Epoch 2 Batch_Num 62 Loss 0.016823258250951767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch_Num 63 Loss 0.012147665955126286\n",
      "Epoch 2 Batch_Num 64 Loss 0.007806837558746338\n",
      "Epoch 2 Batch_Num 65 Loss 0.011053631082177162\n",
      "Epoch 2 Batch_Num 66 Loss 0.012346558272838593\n",
      "Epoch 2 Batch_Num 67 Loss 0.009858084842562675\n",
      "Epoch 2 Batch_Num 68 Loss 0.01796852797269821\n",
      "Epoch 2 Batch_Num 69 Loss 0.01926177181303501\n",
      "Epoch 2 Batch_Num 70 Loss 0.008532140403985977\n",
      "Epoch 2 Batch_Num 71 Loss 0.01133252028375864\n",
      "Epoch 2 Batch_Num 72 Loss 0.008170096203684807\n",
      "Epoch 2 Batch_Num 73 Loss 0.008593777194619179\n",
      "Epoch 2 Batch_Num 74 Loss 0.012897376902401447\n",
      "Epoch 2 Batch_Num 75 Loss 0.01243826188147068\n",
      "Epoch 2 Batch_Num 76 Loss 0.009566245600581169\n",
      "Epoch 2 Batch_Num 77 Loss 0.008790583349764347\n",
      "Epoch 2 Batch_Num 78 Loss 0.016795076429843903\n",
      "Epoch 2 Batch_Num 79 Loss 0.01144393254071474\n",
      "Epoch 2 Batch_Num 80 Loss 0.007827209308743477\n",
      "Epoch 2 Batch_Num 81 Loss 0.009407171979546547\n",
      "Epoch 2 Batch_Num 82 Loss 0.00950551126152277\n",
      "Epoch 2 Batch_Num 83 Loss 0.00976281426846981\n",
      "Epoch 2 Batch_Num 84 Loss 0.010778332129120827\n",
      "Epoch 2 Batch_Num 85 Loss 0.007995921187102795\n",
      "Epoch 2 Batch_Num 86 Loss 0.0062832883559167385\n",
      "Epoch 2 Batch_Num 87 Loss 0.006153334863483906\n",
      "Epoch 2 Batch_Num 88 Loss 0.015117649920284748\n",
      "Epoch 2 Batch_Num 89 Loss 0.005612911656498909\n",
      "Epoch 2 Batch_Num 90 Loss 0.011655853129923344\n",
      "Epoch 2 Batch_Num 91 Loss 0.00640993844717741\n",
      "Epoch 2 Batch_Num 92 Loss 0.015506583265960217\n",
      "Epoch 2 Batch_Num 93 Loss 0.012928517535328865\n",
      "Epoch 2 Batch_Num 94 Loss 0.009310504421591759\n",
      "Epoch 2 Batch_Num 95 Loss 0.003097702981904149\n",
      "Epoch 2 Batch_Num 96 Loss 0.020445138216018677\n",
      "Epoch 2 Batch_Num 97 Loss 0.009150385856628418\n",
      "Epoch 2 Batch_Num 98 Loss 0.009077385067939758\n",
      "Epoch 2 Batch_Num 99 Loss 0.010096065700054169\n",
      "Epoch 2 Batch_Num 100 Loss 0.00882663019001484\n",
      "Epoch 2 Batch_Num 101 Loss 0.010966343805193901\n",
      "Epoch 2 Batch_Num 102 Loss 0.005663180258125067\n",
      "Epoch 2 Batch_Num 103 Loss 0.027823269367218018\n",
      "Epoch 2 Batch_Num 104 Loss 0.02243020199239254\n",
      "Epoch 2 Batch_Num 105 Loss 0.01192449126392603\n",
      "Epoch 2 Batch_Num 106 Loss 0.007529626600444317\n",
      "Epoch 2 Batch_Num 107 Loss 0.014565369114279747\n",
      "Epoch 2 Batch_Num 108 Loss 0.015885036438703537\n",
      "Epoch 2 Batch_Num 109 Loss 0.00664784898981452\n",
      "Epoch 2 Batch_Num 110 Loss 0.0106670456007123\n",
      "Epoch 2 Batch_Num 111 Loss 0.013589834794402122\n",
      "Epoch 2 Batch_Num 112 Loss 0.005525256507098675\n",
      "Epoch 2 Batch_Num 113 Loss 0.009817647747695446\n",
      "Epoch 2 Batch_Num 114 Loss 0.009898751974105835\n",
      "Epoch 2 Batch_Num 115 Loss 0.010381939820945263\n",
      "Epoch 2 Batch_Num 116 Loss 0.015171882696449757\n",
      "Epoch 2 Batch_Num 117 Loss 0.011678246781229973\n",
      "Epoch 2 Batch_Num 118 Loss 0.003567426698282361\n",
      "Epoch 2 Batch_Num 119 Loss 0.014815075322985649\n",
      "Epoch 2 Batch_Num 120 Loss 0.008939223363995552\n",
      "Epoch 2 Batch_Num 121 Loss 0.013633844442665577\n",
      "Epoch 2 Batch_Num 122 Loss 0.010243748314678669\n",
      "Epoch 2 Batch_Num 123 Loss 0.012102547101676464\n",
      "Epoch 2 Batch_Num 124 Loss 0.010532351210713387\n",
      "Epoch 2 Batch_Num 125 Loss 0.009977573528885841\n",
      "Epoch 2 Batch_Num 126 Loss 0.024912238121032715\n",
      "Epoch 2 Batch_Num 127 Loss 0.004854015074670315\n",
      "Epoch 2 Batch_Num 128 Loss 0.008367819711565971\n",
      "Epoch 2 Batch_Num 129 Loss 0.007034058216959238\n",
      "Epoch 2 Batch_Num 130 Loss 0.016445960849523544\n",
      "Epoch 2 Batch_Num 131 Loss 0.009467949159443378\n",
      "Epoch 2 Batch_Num 132 Loss 0.006432893220335245\n",
      "Epoch 2 Batch_Num 133 Loss 0.005620004143565893\n",
      "Epoch 2 Batch_Num 134 Loss 0.015537096187472343\n",
      "Epoch 2 Batch_Num 135 Loss 0.013065236620604992\n",
      "Epoch 2 Batch_Num 136 Loss 0.008668523281812668\n",
      "Epoch 2 Batch_Num 137 Loss 0.012379512190818787\n",
      "Epoch 2 Batch_Num 138 Loss 0.010783086530864239\n",
      "Epoch 2 Batch_Num 139 Loss 0.005591496825218201\n",
      "Epoch 2 Batch_Num 140 Loss 0.014082774519920349\n",
      "Epoch 2 Batch_Num 141 Loss 0.01154125016182661\n",
      "Epoch 2 Batch_Num 142 Loss 0.013276715762913227\n",
      "Epoch 2 Batch_Num 143 Loss 0.013326644897460938\n",
      "Epoch 2 Batch_Num 144 Loss 0.011549079790711403\n",
      "Epoch 2 Batch_Num 145 Loss 0.009934918954968452\n",
      "Epoch 2 Batch_Num 146 Loss 0.013167815282940865\n",
      "Epoch 2 Batch_Num 147 Loss 0.020111678168177605\n",
      "Epoch 2 Batch_Num 148 Loss 0.006536868866533041\n",
      "Epoch 2 Batch_Num 149 Loss 0.010123392567038536\n",
      "Epoch 2 Batch_Num 150 Loss 0.009453082457184792\n",
      "Epoch 2 Batch_Num 151 Loss 0.004285366740077734\n",
      "Epoch 2 Batch_Num 152 Loss 0.004267496522516012\n",
      "Epoch 2 Batch_Num 153 Loss 0.02462131902575493\n",
      "Epoch 2 Batch_Num 154 Loss 0.014474953524768353\n",
      "Epoch 2 Batch_Num 155 Loss 0.009664211422204971\n",
      "Epoch 2 Batch_Num 156 Loss 0.009157981723546982\n",
      "Epoch 2 Batch_Num 157 Loss 0.009555796161293983\n",
      "Epoch 2 Batch_Num 158 Loss 0.006710834801197052\n",
      "Epoch 2 Batch_Num 159 Loss 0.008014922961592674\n",
      "Epoch 2 Batch_Num 160 Loss 0.004273636732250452\n",
      "Epoch 2 Batch_Num 161 Loss 0.020686142146587372\n",
      "Epoch 2 Batch_Num 162 Loss 0.009077774360775948\n",
      "Epoch 2 Batch_Num 163 Loss 0.0134339090436697\n",
      "Epoch 2 Batch_Num 164 Loss 0.01343487948179245\n",
      "Epoch 2 Batch_Num 165 Loss 0.007627081125974655\n",
      "Epoch 2 Batch_Num 166 Loss 0.012017255648970604\n",
      "Epoch 2 Batch_Num 167 Loss 0.01041241455823183\n",
      "Epoch 2 Batch_Num 168 Loss 0.012655978091061115\n",
      "Epoch 2 Batch_Num 169 Loss 0.005835960619151592\n",
      "Epoch 2 Batch_Num 170 Loss 0.014358723536133766\n",
      "Epoch 2 Batch_Num 171 Loss 0.0070993476547300816\n",
      "Epoch 2 Batch_Num 172 Loss 0.009051037952303886\n",
      "Epoch 2 Batch_Num 173 Loss 0.011613705195486546\n",
      "Epoch 2 Batch_Num 174 Loss 0.0047039249911904335\n",
      "Epoch 2 Batch_Num 175 Loss 0.009758256375789642\n",
      "Epoch 2 Batch_Num 176 Loss 0.00692354142665863\n",
      "Epoch 2 Batch_Num 177 Loss 0.01163094025105238\n",
      "Epoch 2 Batch_Num 178 Loss 0.010289860889315605\n",
      "Epoch 2 Batch_Num 179 Loss 0.016866542398929596\n",
      "Epoch 2 Batch_Num 180 Loss 0.014733618125319481\n",
      "Epoch 2 Batch_Num 181 Loss 0.012649953365325928\n",
      "Epoch 2 Batch_Num 182 Loss 0.009563970379531384\n",
      "Epoch 2 Batch_Num 183 Loss 0.010699043050408363\n",
      "Epoch 2 Batch_Num 184 Loss 0.020194094628095627\n",
      "Epoch 2 Batch_Num 185 Loss 0.014565443620085716\n",
      "Epoch 2 Batch_Num 186 Loss 0.010865319520235062\n",
      "Epoch 2 Batch_Num 187 Loss 0.010310737416148186\n",
      "Epoch 2 Batch_Num 188 Loss 0.006597417406737804\n",
      "Epoch 2 Batch_Num 189 Loss 0.0026227938942611217\n",
      "Epoch 2 Batch_Num 190 Loss 0.00395121518522501\n",
      "Epoch 2 Batch_Num 191 Loss 0.014782426878809929\n",
      "Epoch 2 Batch_Num 192 Loss 0.010520515032112598\n",
      "Epoch 2 Batch_Num 193 Loss 0.01879209652543068\n",
      "Epoch 2 Batch_Num 194 Loss 0.01130711566656828\n",
      "Epoch 2 Batch_Num 195 Loss 0.007162271533161402\n",
      "Epoch 2 Batch_Num 196 Loss 0.016136378049850464\n",
      "Epoch 2 Batch_Num 197 Loss 0.004607106558978558\n",
      "Epoch 2 Batch_Num 198 Loss 0.009547853842377663\n",
      "Epoch 2 Batch_Num 199 Loss 0.004341294523328543\n",
      "Epoch 2 Batch_Num 200 Loss 0.004741012584418058\n",
      "Epoch 2 Batch_Num 201 Loss 0.010266104713082314\n",
      "Epoch 2 Batch_Num 202 Loss 0.013270999304950237\n",
      "Epoch 2 Batch_Num 203 Loss 0.014726171270012856\n",
      "Epoch 2 Batch_Num 204 Loss 0.015029300935566425\n",
      "Epoch 2 Batch_Num 205 Loss 0.005647826939821243\n",
      "Epoch 2 Batch_Num 206 Loss 0.023900920525193214\n",
      "Epoch 2 Batch_Num 207 Loss 0.00956893339753151\n",
      "Epoch 2 Batch_Num 208 Loss 0.0103057362139225\n",
      "Epoch 2 Batch_Num 209 Loss 0.010681736283004284\n",
      "Epoch 2 Batch_Num 210 Loss 0.015664462000131607\n",
      "Epoch 2 Batch_Num 211 Loss 0.00986369326710701\n",
      "Epoch 2 Batch_Num 212 Loss 0.010979726910591125\n",
      "Epoch 2 Batch_Num 213 Loss 0.0038325979840010405\n",
      "Epoch 2 Batch_Num 214 Loss 0.0085329320281744\n",
      "Epoch 2 Batch_Num 215 Loss 0.006975152995437384\n",
      "Epoch 2 Batch_Num 216 Loss 0.013428263366222382\n",
      "Epoch 2 Batch_Num 217 Loss 0.010241087526082993\n",
      "Epoch 2 Batch_Num 218 Loss 0.007979290559887886\n",
      "Epoch 2 Batch_Num 219 Loss 0.011068752035498619\n",
      "Epoch 2 Batch_Num 220 Loss 0.0066145481541752815\n",
      "Epoch 2 Batch_Num 221 Loss 0.00900755263864994\n",
      "Epoch 2 Batch_Num 222 Loss 0.005824766121804714\n",
      "Epoch 2 Batch_Num 223 Loss 0.013976086862385273\n",
      "Epoch 2 Batch_Num 224 Loss 0.006206043995916843\n",
      "Epoch 2 Batch_Num 225 Loss 0.01136669609695673\n",
      "Epoch 2 Batch_Num 226 Loss 0.007586047053337097\n",
      "Epoch 2 Batch_Num 227 Loss 0.001503013540059328\n",
      "Epoch 2 Batch_Num 228 Loss 0.0018360381945967674\n",
      "Epoch 2 Batch_Num 229 Loss 0.002754638437181711\n",
      "Epoch 2 Batch_Num 230 Loss 0.00032273452961817384\n",
      "Epoch 2 Batch_Num 231 Loss 0.002397335832938552\n",
      "Epoch 2 Batch_Num 232 Loss 0.005854995455592871\n",
      "Epoch 2 Batch_Num 233 Loss 0.012005528435111046\n",
      "Epoch 2 Batch_Num 234 Loss 0.028342945501208305\n",
      "Epoch 3 Batch_Num 0 Loss 0.01734437607228756\n",
      "Epoch 3 Batch_Num 1 Loss 0.015426714904606342\n",
      "Epoch 3 Batch_Num 2 Loss 0.0099271135404706\n",
      "Epoch 3 Batch_Num 3 Loss 0.015690024942159653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch_Num 4 Loss 0.009663241915404797\n",
      "Epoch 3 Batch_Num 5 Loss 0.015455113723874092\n",
      "Epoch 3 Batch_Num 6 Loss 0.010396640747785568\n",
      "Epoch 3 Batch_Num 7 Loss 0.006710626184940338\n",
      "Epoch 3 Batch_Num 8 Loss 0.008042609319090843\n",
      "Epoch 3 Batch_Num 9 Loss 0.0041933138854801655\n",
      "Epoch 3 Batch_Num 10 Loss 0.013832745142281055\n",
      "Epoch 3 Batch_Num 11 Loss 0.007156206760555506\n",
      "Epoch 3 Batch_Num 12 Loss 0.003021755488589406\n",
      "Epoch 3 Batch_Num 13 Loss 0.008863684721291065\n",
      "Epoch 3 Batch_Num 14 Loss 0.01178536843508482\n",
      "Epoch 3 Batch_Num 15 Loss 0.005417768843472004\n",
      "Epoch 3 Batch_Num 16 Loss 0.0032446631230413914\n",
      "Epoch 3 Batch_Num 17 Loss 0.007614149246364832\n",
      "Epoch 3 Batch_Num 18 Loss 0.006409155670553446\n",
      "Epoch 3 Batch_Num 19 Loss 0.015047302469611168\n",
      "Epoch 3 Batch_Num 20 Loss 0.010636495426297188\n",
      "Epoch 3 Batch_Num 21 Loss 0.004201277159154415\n",
      "Epoch 3 Batch_Num 22 Loss 0.016162442043423653\n",
      "Epoch 3 Batch_Num 23 Loss 0.005925881676375866\n",
      "Epoch 3 Batch_Num 24 Loss 0.007545657455921173\n",
      "Epoch 3 Batch_Num 25 Loss 0.007179323583841324\n",
      "Epoch 3 Batch_Num 26 Loss 0.01103146281093359\n",
      "Epoch 3 Batch_Num 27 Loss 0.012474268674850464\n",
      "Epoch 3 Batch_Num 28 Loss 0.009677162393927574\n",
      "Epoch 3 Batch_Num 29 Loss 0.0052196430042386055\n",
      "Epoch 3 Batch_Num 30 Loss 0.007407348603010178\n",
      "Epoch 3 Batch_Num 31 Loss 0.004202697426080704\n",
      "Epoch 3 Batch_Num 32 Loss 0.01154530793428421\n",
      "Epoch 3 Batch_Num 33 Loss 0.00789480097591877\n",
      "Epoch 3 Batch_Num 34 Loss 0.01665966399013996\n",
      "Epoch 3 Batch_Num 35 Loss 0.005747376009821892\n",
      "Epoch 3 Batch_Num 36 Loss 0.011732787825167179\n",
      "Epoch 3 Batch_Num 37 Loss 0.007281181402504444\n",
      "Epoch 3 Batch_Num 38 Loss 0.006211041938513517\n",
      "Epoch 3 Batch_Num 39 Loss 0.009169922210276127\n",
      "Epoch 3 Batch_Num 40 Loss 0.009249858558177948\n",
      "Epoch 3 Batch_Num 41 Loss 0.00422294344753027\n",
      "Epoch 3 Batch_Num 42 Loss 0.013718897476792336\n",
      "Epoch 3 Batch_Num 43 Loss 0.003325222758576274\n",
      "Epoch 3 Batch_Num 44 Loss 0.001921898452565074\n",
      "Epoch 3 Batch_Num 45 Loss 0.006758715026080608\n",
      "Epoch 3 Batch_Num 46 Loss 0.017559850588440895\n",
      "Epoch 3 Batch_Num 47 Loss 0.004144255071878433\n",
      "Epoch 3 Batch_Num 48 Loss 0.007828181609511375\n",
      "Epoch 3 Batch_Num 49 Loss 0.004326869733631611\n",
      "Epoch 3 Batch_Num 50 Loss 0.010130329988896847\n",
      "Epoch 3 Batch_Num 51 Loss 0.012924807146191597\n",
      "Epoch 3 Batch_Num 52 Loss 0.004267201758921146\n",
      "Epoch 3 Batch_Num 53 Loss 0.007048808969557285\n",
      "Epoch 3 Batch_Num 54 Loss 0.009195765480399132\n",
      "Epoch 3 Batch_Num 55 Loss 0.011720497161149979\n",
      "Epoch 3 Batch_Num 56 Loss 0.012465096078813076\n",
      "Epoch 3 Batch_Num 57 Loss 0.008700864389538765\n",
      "Epoch 3 Batch_Num 58 Loss 0.004501250572502613\n",
      "Epoch 3 Batch_Num 59 Loss 0.013531437143683434\n",
      "Epoch 3 Batch_Num 60 Loss 0.005363945849239826\n",
      "Epoch 3 Batch_Num 61 Loss 0.007594243623316288\n",
      "Epoch 3 Batch_Num 62 Loss 0.006754656787961721\n",
      "Epoch 3 Batch_Num 63 Loss 0.008617415092885494\n",
      "Epoch 3 Batch_Num 64 Loss 0.005113307852298021\n",
      "Epoch 3 Batch_Num 65 Loss 0.008115092292428017\n",
      "Epoch 3 Batch_Num 66 Loss 0.011437306180596352\n",
      "Epoch 3 Batch_Num 67 Loss 0.01063053123652935\n",
      "Epoch 3 Batch_Num 68 Loss 0.013388300314545631\n",
      "Epoch 3 Batch_Num 69 Loss 0.010225685313344002\n",
      "Epoch 3 Batch_Num 70 Loss 0.007336094044148922\n",
      "Epoch 3 Batch_Num 71 Loss 0.008387352339923382\n",
      "Epoch 3 Batch_Num 72 Loss 0.005318404641002417\n",
      "Epoch 3 Batch_Num 73 Loss 0.003199546132236719\n",
      "Epoch 3 Batch_Num 74 Loss 0.006656767334789038\n",
      "Epoch 3 Batch_Num 75 Loss 0.009093139320611954\n",
      "Epoch 3 Batch_Num 76 Loss 0.005442263092845678\n",
      "Epoch 3 Batch_Num 77 Loss 0.0034005269408226013\n",
      "Epoch 3 Batch_Num 78 Loss 0.014157816767692566\n",
      "Epoch 3 Batch_Num 79 Loss 0.008816847577691078\n",
      "Epoch 3 Batch_Num 80 Loss 0.006018758751451969\n",
      "Epoch 3 Batch_Num 81 Loss 0.006028437055647373\n",
      "Epoch 3 Batch_Num 82 Loss 0.006738238036632538\n",
      "Epoch 3 Batch_Num 83 Loss 0.006174071691930294\n",
      "Epoch 3 Batch_Num 84 Loss 0.010248382575809956\n",
      "Epoch 3 Batch_Num 85 Loss 0.0032814834266901016\n",
      "Epoch 3 Batch_Num 86 Loss 0.007674285210669041\n",
      "Epoch 3 Batch_Num 87 Loss 0.006251928862184286\n",
      "Epoch 3 Batch_Num 88 Loss 0.011492418125271797\n",
      "Epoch 3 Batch_Num 89 Loss 0.003182491287589073\n",
      "Epoch 3 Batch_Num 90 Loss 0.011217771098017693\n",
      "Epoch 3 Batch_Num 91 Loss 0.003088412107899785\n",
      "Epoch 3 Batch_Num 92 Loss 0.012904271483421326\n",
      "Epoch 3 Batch_Num 93 Loss 0.0128370001912117\n",
      "Epoch 3 Batch_Num 94 Loss 0.010339836589992046\n",
      "Epoch 3 Batch_Num 95 Loss 0.0023336366284638643\n",
      "Epoch 3 Batch_Num 96 Loss 0.014982764609158039\n",
      "Epoch 3 Batch_Num 97 Loss 0.00599270174279809\n",
      "Epoch 3 Batch_Num 98 Loss 0.007602659519761801\n",
      "Epoch 3 Batch_Num 99 Loss 0.007431847043335438\n",
      "Epoch 3 Batch_Num 100 Loss 0.006635445170104504\n",
      "Epoch 3 Batch_Num 101 Loss 0.004337563179433346\n",
      "Epoch 3 Batch_Num 102 Loss 0.0026736564468592405\n",
      "Epoch 3 Batch_Num 103 Loss 0.02231268584728241\n",
      "Epoch 3 Batch_Num 104 Loss 0.016500547528266907\n",
      "Epoch 3 Batch_Num 105 Loss 0.011067183688282967\n",
      "Epoch 3 Batch_Num 106 Loss 0.0057533252984285355\n",
      "Epoch 3 Batch_Num 107 Loss 0.011761795729398727\n",
      "Epoch 3 Batch_Num 108 Loss 0.00981314666569233\n",
      "Epoch 3 Batch_Num 109 Loss 0.0036085452884435654\n",
      "Epoch 3 Batch_Num 110 Loss 0.011442157439887524\n",
      "Epoch 3 Batch_Num 111 Loss 0.008196292445063591\n",
      "Epoch 3 Batch_Num 112 Loss 0.003619800554588437\n",
      "Epoch 3 Batch_Num 113 Loss 0.007292106747627258\n",
      "Epoch 3 Batch_Num 114 Loss 0.009331836365163326\n",
      "Epoch 3 Batch_Num 115 Loss 0.007062904536724091\n",
      "Epoch 3 Batch_Num 116 Loss 0.009340417571365833\n",
      "Epoch 3 Batch_Num 117 Loss 0.010391317307949066\n",
      "Epoch 3 Batch_Num 118 Loss 0.0018810549518093467\n",
      "Epoch 3 Batch_Num 119 Loss 0.008154112845659256\n",
      "Epoch 3 Batch_Num 120 Loss 0.005839409772306681\n",
      "Epoch 3 Batch_Num 121 Loss 0.01046588271856308\n",
      "Epoch 3 Batch_Num 122 Loss 0.006211067549884319\n",
      "Epoch 3 Batch_Num 123 Loss 0.009852333925664425\n",
      "Epoch 3 Batch_Num 124 Loss 0.008421818725764751\n",
      "Epoch 3 Batch_Num 125 Loss 0.006499060895293951\n",
      "Epoch 3 Batch_Num 126 Loss 0.019681306555867195\n",
      "Epoch 3 Batch_Num 127 Loss 0.005176445934921503\n",
      "Epoch 3 Batch_Num 128 Loss 0.005550882313400507\n",
      "Epoch 3 Batch_Num 129 Loss 0.0035344951320439577\n",
      "Epoch 3 Batch_Num 130 Loss 0.007713527884334326\n",
      "Epoch 3 Batch_Num 131 Loss 0.0039483108557760715\n",
      "Epoch 3 Batch_Num 132 Loss 0.004087178967893124\n",
      "Epoch 3 Batch_Num 133 Loss 0.004652688279747963\n",
      "Epoch 3 Batch_Num 134 Loss 0.012096621096134186\n",
      "Epoch 3 Batch_Num 135 Loss 0.008550350554287434\n",
      "Epoch 3 Batch_Num 136 Loss 0.012138515710830688\n",
      "Epoch 3 Batch_Num 137 Loss 0.0088582094758749\n",
      "Epoch 3 Batch_Num 138 Loss 0.007407204248011112\n",
      "Epoch 3 Batch_Num 139 Loss 0.00570259103551507\n",
      "Epoch 3 Batch_Num 140 Loss 0.006784378085285425\n",
      "Epoch 3 Batch_Num 141 Loss 0.006013228092342615\n",
      "Epoch 3 Batch_Num 142 Loss 0.006942713167518377\n",
      "Epoch 3 Batch_Num 143 Loss 0.0066949548199772835\n",
      "Epoch 3 Batch_Num 144 Loss 0.007297494448721409\n",
      "Epoch 3 Batch_Num 145 Loss 0.006673441268503666\n",
      "Epoch 3 Batch_Num 146 Loss 0.010467806831002235\n",
      "Epoch 3 Batch_Num 147 Loss 0.01630260981619358\n",
      "Epoch 3 Batch_Num 148 Loss 0.002678392454981804\n",
      "Epoch 3 Batch_Num 149 Loss 0.007376362569630146\n",
      "Epoch 3 Batch_Num 150 Loss 0.00938631221652031\n",
      "Epoch 3 Batch_Num 151 Loss 0.004250430501997471\n",
      "Epoch 3 Batch_Num 152 Loss 0.004552613012492657\n",
      "Epoch 3 Batch_Num 153 Loss 0.017373908311128616\n",
      "Epoch 3 Batch_Num 154 Loss 0.007230819668620825\n",
      "Epoch 3 Batch_Num 155 Loss 0.007395290769636631\n",
      "Epoch 3 Batch_Num 156 Loss 0.010743044316768646\n",
      "Epoch 3 Batch_Num 157 Loss 0.008986817672848701\n",
      "Epoch 3 Batch_Num 158 Loss 0.004474300425499678\n",
      "Epoch 3 Batch_Num 159 Loss 0.0037910672836005688\n",
      "Epoch 3 Batch_Num 160 Loss 0.0024537488352507353\n",
      "Epoch 3 Batch_Num 161 Loss 0.014032339677214622\n",
      "Epoch 3 Batch_Num 162 Loss 0.016840049996972084\n",
      "Epoch 3 Batch_Num 163 Loss 0.009435469284653664\n",
      "Epoch 3 Batch_Num 164 Loss 0.01028518658131361\n",
      "Epoch 3 Batch_Num 165 Loss 0.004008247517049313\n",
      "Epoch 3 Batch_Num 166 Loss 0.007434104569256306\n",
      "Epoch 3 Batch_Num 167 Loss 0.007332792039960623\n",
      "Epoch 3 Batch_Num 168 Loss 0.01411929540336132\n",
      "Epoch 3 Batch_Num 169 Loss 0.008811308071017265\n",
      "Epoch 3 Batch_Num 170 Loss 0.008876116015017033\n",
      "Epoch 3 Batch_Num 171 Loss 0.005688037723302841\n",
      "Epoch 3 Batch_Num 172 Loss 0.0076409028843045235\n",
      "Epoch 3 Batch_Num 173 Loss 0.010741432197391987\n",
      "Epoch 3 Batch_Num 174 Loss 0.0022618044167757034\n",
      "Epoch 3 Batch_Num 175 Loss 0.005907815415412188\n",
      "Epoch 3 Batch_Num 176 Loss 0.003964873496443033\n",
      "Epoch 3 Batch_Num 177 Loss 0.009636339731514454\n",
      "Epoch 3 Batch_Num 178 Loss 0.005794450640678406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch_Num 179 Loss 0.014331248588860035\n",
      "Epoch 3 Batch_Num 180 Loss 0.013866022229194641\n",
      "Epoch 3 Batch_Num 181 Loss 0.01050182618200779\n",
      "Epoch 3 Batch_Num 182 Loss 0.005698611494153738\n",
      "Epoch 3 Batch_Num 183 Loss 0.004127605818212032\n",
      "Epoch 3 Batch_Num 184 Loss 0.015164142474532127\n",
      "Epoch 3 Batch_Num 185 Loss 0.012089858762919903\n",
      "Epoch 3 Batch_Num 186 Loss 0.010891219601035118\n",
      "Epoch 3 Batch_Num 187 Loss 0.006501484662294388\n",
      "Epoch 3 Batch_Num 188 Loss 0.005093714222311974\n",
      "Epoch 3 Batch_Num 189 Loss 0.0015354815404862165\n",
      "Epoch 3 Batch_Num 190 Loss 0.0011830620933324099\n",
      "Epoch 3 Batch_Num 191 Loss 0.01170666329562664\n",
      "Epoch 3 Batch_Num 192 Loss 0.00863683968782425\n",
      "Epoch 3 Batch_Num 193 Loss 0.014068658463656902\n",
      "Epoch 3 Batch_Num 194 Loss 0.006238905247300863\n",
      "Epoch 3 Batch_Num 195 Loss 0.005628342740237713\n",
      "Epoch 3 Batch_Num 196 Loss 0.014674303121864796\n",
      "Epoch 3 Batch_Num 197 Loss 0.004165785852819681\n",
      "Epoch 3 Batch_Num 198 Loss 0.008435399271547794\n",
      "Epoch 3 Batch_Num 199 Loss 0.0030698596965521574\n",
      "Epoch 3 Batch_Num 200 Loss 0.006069989409297705\n",
      "Epoch 3 Batch_Num 201 Loss 0.007348323706537485\n",
      "Epoch 3 Batch_Num 202 Loss 0.01164480485022068\n",
      "Epoch 3 Batch_Num 203 Loss 0.011849861592054367\n",
      "Epoch 3 Batch_Num 204 Loss 0.012778474017977715\n",
      "Epoch 3 Batch_Num 205 Loss 0.009697332046926022\n",
      "Epoch 3 Batch_Num 206 Loss 0.015399878844618797\n",
      "Epoch 3 Batch_Num 207 Loss 0.009057024493813515\n",
      "Epoch 3 Batch_Num 208 Loss 0.008330006152391434\n",
      "Epoch 3 Batch_Num 209 Loss 0.008414478041231632\n",
      "Epoch 3 Batch_Num 210 Loss 0.009533119387924671\n",
      "Epoch 3 Batch_Num 211 Loss 0.006387862376868725\n",
      "Epoch 3 Batch_Num 212 Loss 0.009926744736731052\n",
      "Epoch 3 Batch_Num 213 Loss 0.004074577242136002\n",
      "Epoch 3 Batch_Num 214 Loss 0.004910898394882679\n",
      "Epoch 3 Batch_Num 215 Loss 0.005049333442002535\n",
      "Epoch 3 Batch_Num 216 Loss 0.011590211652219296\n",
      "Epoch 3 Batch_Num 217 Loss 0.008857214823365211\n",
      "Epoch 3 Batch_Num 218 Loss 0.010291625745594501\n",
      "Epoch 3 Batch_Num 219 Loss 0.007415025029331446\n",
      "Epoch 3 Batch_Num 220 Loss 0.005025154445320368\n",
      "Epoch 3 Batch_Num 221 Loss 0.005537889897823334\n",
      "Epoch 3 Batch_Num 222 Loss 0.005570757202804089\n",
      "Epoch 3 Batch_Num 223 Loss 0.005198890808969736\n",
      "Epoch 3 Batch_Num 224 Loss 0.004625992849469185\n",
      "Epoch 3 Batch_Num 225 Loss 0.01111685298383236\n",
      "Epoch 3 Batch_Num 226 Loss 0.005860889796167612\n",
      "Epoch 3 Batch_Num 227 Loss 0.0006082055624574423\n",
      "Epoch 3 Batch_Num 228 Loss 0.0011333739385008812\n",
      "Epoch 3 Batch_Num 229 Loss 0.00379173387773335\n",
      "Epoch 3 Batch_Num 230 Loss 0.0002764803939498961\n",
      "Epoch 3 Batch_Num 231 Loss 0.003050032304599881\n",
      "Epoch 3 Batch_Num 232 Loss 0.00904347375035286\n",
      "Epoch 3 Batch_Num 233 Loss 0.012640130706131458\n",
      "Epoch 3 Batch_Num 234 Loss 0.023289082571864128\n",
      "Epoch 4 Batch_Num 0 Loss 0.011965076439082623\n",
      "Epoch 4 Batch_Num 1 Loss 0.013220680877566338\n",
      "Epoch 4 Batch_Num 2 Loss 0.00959664024412632\n",
      "Epoch 4 Batch_Num 3 Loss 0.013976329937577248\n",
      "Epoch 4 Batch_Num 4 Loss 0.010180609300732613\n",
      "Epoch 4 Batch_Num 5 Loss 0.013943949714303017\n",
      "Epoch 4 Batch_Num 6 Loss 0.01154378242790699\n",
      "Epoch 4 Batch_Num 7 Loss 0.003199248807504773\n",
      "Epoch 4 Batch_Num 8 Loss 0.0058015794493258\n",
      "Epoch 4 Batch_Num 9 Loss 0.0037463493645191193\n",
      "Epoch 4 Batch_Num 10 Loss 0.01569652184844017\n",
      "Epoch 4 Batch_Num 11 Loss 0.007603093050420284\n",
      "Epoch 4 Batch_Num 12 Loss 0.0022367897909134626\n",
      "Epoch 4 Batch_Num 13 Loss 0.011043727397918701\n",
      "Epoch 4 Batch_Num 14 Loss 0.011253152042627335\n",
      "Epoch 4 Batch_Num 15 Loss 0.00597838219255209\n",
      "Epoch 4 Batch_Num 16 Loss 0.003822689177468419\n",
      "Epoch 4 Batch_Num 17 Loss 0.0061190188862383366\n",
      "Epoch 4 Batch_Num 18 Loss 0.0032099937088787556\n",
      "Epoch 4 Batch_Num 19 Loss 0.013963676989078522\n",
      "Epoch 4 Batch_Num 20 Loss 0.014979277737438679\n",
      "Epoch 4 Batch_Num 21 Loss 0.00180727057158947\n",
      "Epoch 4 Batch_Num 22 Loss 0.012833625078201294\n",
      "Epoch 4 Batch_Num 23 Loss 0.0035032988525927067\n",
      "Epoch 4 Batch_Num 24 Loss 0.010451765730977058\n",
      "Epoch 4 Batch_Num 25 Loss 0.008782590739428997\n",
      "Epoch 4 Batch_Num 26 Loss 0.007266867905855179\n",
      "Epoch 4 Batch_Num 27 Loss 0.005215151235461235\n",
      "Epoch 4 Batch_Num 28 Loss 0.013285812921822071\n",
      "Epoch 4 Batch_Num 29 Loss 0.0038951816968619823\n",
      "Epoch 4 Batch_Num 30 Loss 0.006243621464818716\n",
      "Epoch 4 Batch_Num 31 Loss 0.004107826855033636\n",
      "Epoch 4 Batch_Num 32 Loss 0.011356962844729424\n",
      "Epoch 4 Batch_Num 33 Loss 0.009307818487286568\n",
      "Epoch 4 Batch_Num 34 Loss 0.013610060326755047\n",
      "Epoch 4 Batch_Num 35 Loss 0.005007357802242041\n",
      "Epoch 4 Batch_Num 36 Loss 0.013749942183494568\n",
      "Epoch 4 Batch_Num 37 Loss 0.005912302527576685\n",
      "Epoch 4 Batch_Num 38 Loss 0.006123539060354233\n",
      "Epoch 4 Batch_Num 39 Loss 0.007407521363347769\n",
      "Epoch 4 Batch_Num 40 Loss 0.011709257028996944\n",
      "Epoch 4 Batch_Num 41 Loss 0.002987062791362405\n",
      "Epoch 4 Batch_Num 42 Loss 0.012036408297717571\n",
      "Epoch 4 Batch_Num 43 Loss 0.0033617254812270403\n",
      "Epoch 4 Batch_Num 44 Loss 0.0027429924812167883\n",
      "Epoch 4 Batch_Num 45 Loss 0.006144512910395861\n",
      "Epoch 4 Batch_Num 46 Loss 0.013879996724426746\n",
      "Epoch 4 Batch_Num 47 Loss 0.0022677637171000242\n",
      "Epoch 4 Batch_Num 48 Loss 0.00235350476577878\n",
      "Epoch 4 Batch_Num 49 Loss 0.0043975175358355045\n",
      "Epoch 4 Batch_Num 50 Loss 0.008717084303498268\n",
      "Epoch 4 Batch_Num 51 Loss 0.010728439316153526\n",
      "Epoch 4 Batch_Num 52 Loss 0.005297652445733547\n",
      "Epoch 4 Batch_Num 53 Loss 0.0035965375136584044\n",
      "Epoch 4 Batch_Num 54 Loss 0.008818179368972778\n",
      "Epoch 4 Batch_Num 55 Loss 0.0064684548415243626\n",
      "Epoch 4 Batch_Num 56 Loss 0.010462457314133644\n",
      "Epoch 4 Batch_Num 57 Loss 0.006444933824241161\n",
      "Epoch 4 Batch_Num 58 Loss 0.002094161929562688\n",
      "Epoch 4 Batch_Num 59 Loss 0.007120226975530386\n",
      "Epoch 4 Batch_Num 60 Loss 0.006170113105326891\n",
      "Epoch 4 Batch_Num 61 Loss 0.0073248641565442085\n",
      "Epoch 4 Batch_Num 62 Loss 0.005542543716728687\n",
      "Epoch 4 Batch_Num 63 Loss 0.006785612553358078\n",
      "Epoch 4 Batch_Num 64 Loss 0.00426903972402215\n",
      "Epoch 4 Batch_Num 65 Loss 0.00884261541068554\n",
      "Epoch 4 Batch_Num 66 Loss 0.005652640946209431\n",
      "Epoch 4 Batch_Num 67 Loss 0.0063344636000692844\n",
      "Epoch 4 Batch_Num 68 Loss 0.007904574275016785\n",
      "Epoch 4 Batch_Num 69 Loss 0.004557312466204166\n",
      "Epoch 4 Batch_Num 70 Loss 0.0075335269793868065\n",
      "Epoch 4 Batch_Num 71 Loss 0.00957482773810625\n",
      "Epoch 4 Batch_Num 72 Loss 0.0031643423717468977\n",
      "Epoch 4 Batch_Num 73 Loss 0.002389380242675543\n",
      "Epoch 4 Batch_Num 74 Loss 0.007511494215577841\n",
      "Epoch 4 Batch_Num 75 Loss 0.00745793292298913\n",
      "Epoch 4 Batch_Num 76 Loss 0.0028914702124893665\n",
      "Epoch 4 Batch_Num 77 Loss 0.0032757099252194166\n",
      "Epoch 4 Batch_Num 78 Loss 0.01578456163406372\n",
      "Epoch 4 Batch_Num 79 Loss 0.0055448818020522594\n",
      "Epoch 4 Batch_Num 80 Loss 0.005687507800757885\n",
      "Epoch 4 Batch_Num 81 Loss 0.006314984522759914\n",
      "Epoch 4 Batch_Num 82 Loss 0.007573594339191914\n",
      "Epoch 4 Batch_Num 83 Loss 0.004462881945073605\n",
      "Epoch 4 Batch_Num 84 Loss 0.01181707065552473\n",
      "Epoch 4 Batch_Num 85 Loss 0.004907251335680485\n",
      "Epoch 4 Batch_Num 86 Loss 0.003718166844919324\n",
      "Epoch 4 Batch_Num 87 Loss 0.0061229923740029335\n",
      "Epoch 4 Batch_Num 88 Loss 0.007501531392335892\n",
      "Epoch 4 Batch_Num 89 Loss 0.0014727257657796144\n",
      "Epoch 4 Batch_Num 90 Loss 0.006604363210499287\n",
      "Epoch 4 Batch_Num 91 Loss 0.003074989188462496\n",
      "Epoch 4 Batch_Num 92 Loss 0.015479902736842632\n",
      "Epoch 4 Batch_Num 93 Loss 0.006778922863304615\n",
      "Epoch 4 Batch_Num 94 Loss 0.007339458912611008\n",
      "Epoch 4 Batch_Num 95 Loss 0.0012938835425302386\n",
      "Epoch 4 Batch_Num 96 Loss 0.007113408297300339\n",
      "Epoch 4 Batch_Num 97 Loss 0.003688185242936015\n",
      "Epoch 4 Batch_Num 98 Loss 0.003426241921260953\n",
      "Epoch 4 Batch_Num 99 Loss 0.008215506561100483\n",
      "Epoch 4 Batch_Num 100 Loss 0.007881512865424156\n",
      "Epoch 4 Batch_Num 101 Loss 0.003159628016874194\n",
      "Epoch 4 Batch_Num 102 Loss 0.0020864042453467846\n",
      "Epoch 4 Batch_Num 103 Loss 0.019968142732977867\n",
      "Epoch 4 Batch_Num 104 Loss 0.014167864806950092\n",
      "Epoch 4 Batch_Num 105 Loss 0.009751184843480587\n",
      "Epoch 4 Batch_Num 106 Loss 0.004143779631704092\n",
      "Epoch 4 Batch_Num 107 Loss 0.010174809023737907\n",
      "Epoch 4 Batch_Num 108 Loss 0.009241582825779915\n",
      "Epoch 4 Batch_Num 109 Loss 0.0068229250609874725\n",
      "Epoch 4 Batch_Num 110 Loss 0.006560345645993948\n",
      "Epoch 4 Batch_Num 111 Loss 0.009450895711779594\n",
      "Epoch 4 Batch_Num 112 Loss 0.005784048233181238\n",
      "Epoch 4 Batch_Num 113 Loss 0.007467017974704504\n",
      "Epoch 4 Batch_Num 114 Loss 0.004754961468279362\n",
      "Epoch 4 Batch_Num 115 Loss 0.004732432775199413\n",
      "Epoch 4 Batch_Num 116 Loss 0.005187549628317356\n",
      "Epoch 4 Batch_Num 117 Loss 0.00373985362239182\n",
      "Epoch 4 Batch_Num 118 Loss 0.0011752367718145251\n",
      "Epoch 4 Batch_Num 119 Loss 0.0049712928012013435\n",
      "Epoch 4 Batch_Num 120 Loss 0.014100496657192707\n",
      "Epoch 4 Batch_Num 121 Loss 0.0071258582174777985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch_Num 122 Loss 0.0053812176920473576\n",
      "Epoch 4 Batch_Num 123 Loss 0.008988218382000923\n",
      "Epoch 4 Batch_Num 124 Loss 0.005872935988008976\n",
      "Epoch 4 Batch_Num 125 Loss 0.0075286077335476875\n",
      "Epoch 4 Batch_Num 126 Loss 0.01389746367931366\n",
      "Epoch 4 Batch_Num 127 Loss 0.004240592010319233\n",
      "Epoch 4 Batch_Num 128 Loss 0.0038061223458498716\n",
      "Epoch 4 Batch_Num 129 Loss 0.0015015447279438376\n",
      "Epoch 4 Batch_Num 130 Loss 0.00677782017737627\n",
      "Epoch 4 Batch_Num 131 Loss 0.003685213625431061\n",
      "Epoch 4 Batch_Num 132 Loss 0.0026299457531422377\n",
      "Epoch 4 Batch_Num 133 Loss 0.004336022771894932\n",
      "Epoch 4 Batch_Num 134 Loss 0.010173095390200615\n",
      "Epoch 4 Batch_Num 135 Loss 0.00703784916549921\n",
      "Epoch 4 Batch_Num 136 Loss 0.007286728825420141\n",
      "Epoch 4 Batch_Num 137 Loss 0.008681156672537327\n",
      "Epoch 4 Batch_Num 138 Loss 0.0125799011439085\n",
      "Epoch 4 Batch_Num 139 Loss 0.0054532615467906\n",
      "Epoch 4 Batch_Num 140 Loss 0.003713936312124133\n",
      "Epoch 4 Batch_Num 141 Loss 0.003910969011485577\n",
      "Epoch 4 Batch_Num 142 Loss 0.008589892648160458\n",
      "Epoch 4 Batch_Num 143 Loss 0.011283514089882374\n",
      "Epoch 4 Batch_Num 144 Loss 0.006623705383390188\n",
      "Epoch 4 Batch_Num 145 Loss 0.005747768562287092\n",
      "Epoch 4 Batch_Num 146 Loss 0.00449929665774107\n",
      "Epoch 4 Batch_Num 147 Loss 0.015215689316391945\n",
      "Epoch 4 Batch_Num 148 Loss 0.002562191803008318\n",
      "Epoch 4 Batch_Num 149 Loss 0.00863555446267128\n",
      "Epoch 4 Batch_Num 150 Loss 0.011224804446101189\n",
      "Epoch 4 Batch_Num 151 Loss 0.00720905140042305\n",
      "Epoch 4 Batch_Num 152 Loss 0.0024628709070384502\n",
      "Epoch 4 Batch_Num 153 Loss 0.019960038363933563\n",
      "Epoch 4 Batch_Num 154 Loss 0.005129203200340271\n",
      "Epoch 4 Batch_Num 155 Loss 0.007242746651172638\n",
      "Epoch 4 Batch_Num 156 Loss 0.009758183732628822\n",
      "Epoch 4 Batch_Num 157 Loss 0.011597709730267525\n",
      "Epoch 4 Batch_Num 158 Loss 0.00881428737193346\n",
      "Epoch 4 Batch_Num 159 Loss 0.00730407889932394\n",
      "Epoch 4 Batch_Num 160 Loss 0.006292989943176508\n",
      "Epoch 4 Batch_Num 161 Loss 0.02028629742562771\n",
      "Epoch 4 Batch_Num 162 Loss 0.007189588155597448\n",
      "Epoch 4 Batch_Num 163 Loss 0.010958584025502205\n",
      "Epoch 4 Batch_Num 164 Loss 0.003864916041493416\n",
      "Epoch 4 Batch_Num 165 Loss 0.0036037813406437635\n",
      "Epoch 4 Batch_Num 166 Loss 0.012668265029788017\n",
      "Epoch 4 Batch_Num 167 Loss 0.007347849663347006\n",
      "Epoch 4 Batch_Num 168 Loss 0.011491439305245876\n",
      "Epoch 4 Batch_Num 169 Loss 0.00549247395247221\n",
      "Epoch 4 Batch_Num 170 Loss 0.011977436020970345\n",
      "Epoch 4 Batch_Num 171 Loss 0.0046464246697723866\n",
      "Epoch 4 Batch_Num 172 Loss 0.006890830583870411\n",
      "Epoch 4 Batch_Num 173 Loss 0.007933182641863823\n",
      "Epoch 4 Batch_Num 174 Loss 0.0028176014311611652\n",
      "Epoch 4 Batch_Num 175 Loss 0.006750076077878475\n",
      "Epoch 4 Batch_Num 176 Loss 0.0024368215817958117\n",
      "Epoch 4 Batch_Num 177 Loss 0.007230599410831928\n",
      "Epoch 4 Batch_Num 178 Loss 0.008769581094384193\n",
      "Epoch 4 Batch_Num 179 Loss 0.011590173467993736\n",
      "Epoch 4 Batch_Num 180 Loss 0.010566351935267448\n",
      "Epoch 4 Batch_Num 181 Loss 0.004405447747558355\n",
      "Epoch 4 Batch_Num 182 Loss 0.005302115343511105\n",
      "Epoch 4 Batch_Num 183 Loss 0.0054249526001513\n",
      "Epoch 4 Batch_Num 184 Loss 0.010428791865706444\n",
      "Epoch 4 Batch_Num 185 Loss 0.014961415901780128\n",
      "Epoch 4 Batch_Num 186 Loss 0.007518602069467306\n",
      "Epoch 4 Batch_Num 187 Loss 0.0073113152757287025\n",
      "Epoch 4 Batch_Num 188 Loss 0.008539233356714249\n",
      "Epoch 4 Batch_Num 189 Loss 0.0023274242412298918\n",
      "Epoch 4 Batch_Num 190 Loss 0.0013825150672346354\n",
      "Epoch 4 Batch_Num 191 Loss 0.00987257994711399\n",
      "Epoch 4 Batch_Num 192 Loss 0.007576668169349432\n",
      "Epoch 4 Batch_Num 193 Loss 0.01094692200422287\n",
      "Epoch 4 Batch_Num 194 Loss 0.0054178861901164055\n",
      "Epoch 4 Batch_Num 195 Loss 0.007557024713605642\n",
      "Epoch 4 Batch_Num 196 Loss 0.014829671010375023\n",
      "Epoch 4 Batch_Num 197 Loss 0.004284624010324478\n",
      "Epoch 4 Batch_Num 198 Loss 0.013250589370727539\n",
      "Epoch 4 Batch_Num 199 Loss 0.0017936885124072433\n",
      "Epoch 4 Batch_Num 200 Loss 0.004871977027505636\n",
      "Epoch 4 Batch_Num 201 Loss 0.0037614721804857254\n",
      "Epoch 4 Batch_Num 202 Loss 0.008527649566531181\n",
      "Epoch 4 Batch_Num 203 Loss 0.009536045603454113\n",
      "Epoch 4 Batch_Num 204 Loss 0.009408866055309772\n",
      "Epoch 4 Batch_Num 205 Loss 0.004833133425563574\n",
      "Epoch 4 Batch_Num 206 Loss 0.011589045636355877\n",
      "Epoch 4 Batch_Num 207 Loss 0.007802117615938187\n",
      "Epoch 4 Batch_Num 208 Loss 0.005977855995297432\n",
      "Epoch 4 Batch_Num 209 Loss 0.008407128974795341\n",
      "Epoch 4 Batch_Num 210 Loss 0.007082725875079632\n",
      "Epoch 4 Batch_Num 211 Loss 0.008577169850468636\n",
      "Epoch 4 Batch_Num 212 Loss 0.0037445444613695145\n",
      "Epoch 4 Batch_Num 213 Loss 0.003820066573098302\n",
      "Epoch 4 Batch_Num 214 Loss 0.007093449123203754\n",
      "Epoch 4 Batch_Num 215 Loss 0.00240335613489151\n",
      "Epoch 4 Batch_Num 216 Loss 0.007205503527075052\n",
      "Epoch 4 Batch_Num 217 Loss 0.010690264403820038\n",
      "Epoch 4 Batch_Num 218 Loss 0.007954970002174377\n",
      "Epoch 4 Batch_Num 219 Loss 0.00497764628380537\n",
      "Epoch 4 Batch_Num 220 Loss 0.005693626124411821\n",
      "Epoch 4 Batch_Num 221 Loss 0.007187464274466038\n",
      "Epoch 4 Batch_Num 222 Loss 0.003713794518262148\n",
      "Epoch 4 Batch_Num 223 Loss 0.005730119533836842\n",
      "Epoch 4 Batch_Num 224 Loss 0.004050048999488354\n",
      "Epoch 4 Batch_Num 225 Loss 0.006920917425304651\n",
      "Epoch 4 Batch_Num 226 Loss 0.0020205567125231028\n",
      "Epoch 4 Batch_Num 227 Loss 0.0003390136989764869\n",
      "Epoch 4 Batch_Num 228 Loss 0.0011678982991725206\n",
      "Epoch 4 Batch_Num 229 Loss 0.0032226741313934326\n",
      "Epoch 4 Batch_Num 230 Loss 0.00047049159184098244\n",
      "Epoch 4 Batch_Num 231 Loss 0.0007975655607879162\n",
      "Epoch 4 Batch_Num 232 Loss 0.0021923333406448364\n",
      "Epoch 4 Batch_Num 233 Loss 0.010345123708248138\n",
      "Epoch 4 Batch_Num 234 Loss 0.01881873980164528\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for f in range(num_epochs):\n",
    "    for batch_num, minibatch in enumerate(train_loader): \n",
    "        minibatch_x, minibatch_y = minibatch[0], minibatch[1]\n",
    "        \n",
    "        output = model.forward(torch.Tensor(minibatch_x.float()))\n",
    "        loss = criterion(output, torch.Tensor(minibatch_y.float()))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {f} Batch_Num {batch_num} Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5935e6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/ipykernel_49784/2121290085.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = nn.Softmax()(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_acc:  0.9779\n",
      "auc_score:  0.9878326743081736\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"PyTorch_MNIST\") \n",
    "\n",
    "with mlflow.start_run():\n",
    "    preds = model.forward(torch.Tensor(x_test.float()))\n",
    "    preds = np.round(preds.detach().cpu().numpy())\n",
    "    \n",
    "    eval_acc = accuracy_score(y_test, preds)\n",
    "    auc_score = roc_auc_score(y_test, preds)\n",
    "    \n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    \n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"auc_score\", auc_score)\n",
    "    \n",
    "    print(\"eval_acc: \", eval_acc)\n",
    "    print(\"auc_score: \", auc_score)\n",
    "    mlflow.pytorch.log_model(model, \"PyTorch_MNIST\")\n",
    "    \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61cad0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pytorch.load_model(\"runs:/5baeb13b3b29422f98ab87a712677199/PyTorch_MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee9ba5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_acc:  0.9779\n",
      "auc_score:  0.9878326743081736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yc/l505mhd94ws7bth73k_v0xkr0000gn/T/ipykernel_49784/2121290085.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = nn.Softmax()(x)\n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.forward(torch.Tensor(x_test.float()))\n",
    "preds = np.round(preds.detach().cpu().numpy())\n",
    "eval_acc = accuracy_score(y_test, preds)\n",
    "auc_score = roc_auc_score(y_test, preds)\n",
    "print(\"eval_acc: \", eval_acc)\n",
    "print(\"auc_score: \", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19e304",
   "metadata": {},
   "source": [
    "# MLFlow with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9a31b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark #\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sklearn #\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import matplotlib #\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow \n",
    "import mlflow.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72cac69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 23:52:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/03 23:52:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark: 3.1.2\n",
      "matplotlib: 3.4.3\n",
      "seaborn: 0.11.2\n",
      "sklearn: 0.24.2\n",
      "mlflow: 1.20.2\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"SPARK_LOCAL_IP\"]='127.0.0.1'\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext._conf.getAll()\n",
    "print(\"pyspark: {}\".format(pyspark.__version__))\n",
    "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
    "print(\"seaborn: {}\".format(sns.__version__))\n",
    "print(\"sklearn: {}\".format(sklearn.__version__))\n",
    "print(\"mlflow: {}\".format(mlflow.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5bd2792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                        (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_path = 'data/creditcard.csv'\n",
    "df = spark.read.csv(data_path, header = True, inferSchema = True)\n",
    "labelColumn = \"Class\"\n",
    "columns = df.columns\n",
    "numericCols = columns\n",
    "numericCols.remove(\"Time\")\n",
    "numericCols.remove(labelColumn)\n",
    "print(numericCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d27da2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 23:53:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfdf6c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "assemblerInputs =  numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs,outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "dfFeatures = df.select(F.col(labelColumn).alias('label'), *numericCols )\n",
    "normal = dfFeatures.filter(\"Class == 0\"). sample(withReplacement=False, fraction=0.5, seed=2020) \n",
    "anomaly = dfFeatures.filter(\"Class == 1\")\n",
    "normal_train, normal_test = normal.randomSplit([0.8, 0.2], seed = 2020)\n",
    "anomaly_train, anomaly_test = anomaly.randomSplit([0.8, 0.2], seed = 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d8696ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        V1        V2        V3        V4        V5        V6  \\\n",
       "0      0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1      0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2      0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3      0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4      0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0  0.239599  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.791461  0.247676 -1.514654  ...  0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFeatures.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54bd2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = normal_train.union(anomaly_train)\n",
    "test_set = normal_test.union(anomaly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cc2c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count:  114161\n",
      "Test Dataset Count:  28465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:=============================>                          (17 + 15) / 32]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(dfFeatures)\n",
    "train_set = pipelineModel.transform(train_set)\n",
    "test_set = pipelineModel.transform(test_set)\n",
    "selectedCols = ['label', 'features'] + numericCols\n",
    "train_set = train_set.select(selectedCols)\n",
    "test_set = test_set.select(selectedCols)\n",
    "print(\"Training Dataset Count: \", train_set.count())\n",
    "print(\"Test Dataset Count: \", test_set.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aaddd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(spark_model, train_set): \n",
    "    trained_model = spark_model.fit(train_set)\n",
    "    trainingSummary = trained_model.summary\n",
    "    pyspark_auc_score = trainingSummary.areaUnderROC\n",
    "    mlflow.log_metric(\"train_acc\", trainingSummary.accuracy)\n",
    "    mlflow.log_metric(\"train_AUC\", pyspark_auc_score)\n",
    "    print(\"Training Accuracy: \", trainingSummary.accuracy)\n",
    "    print(\"Training AUC:\", pyspark_auc_score)\n",
    "    \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f87282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(spark_model, test_set):\n",
    "    \n",
    "    evaluation_summary = spark_model.evaluate(test_set)\n",
    "    \n",
    "    eval_acc = evaluation_summary.accuracy\n",
    "    eval_AUC = evaluation_summary.areaUnderROC\n",
    "    \n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"eval_AUC\", eval_AUC)\n",
    "    \n",
    "    print(\"Evaluation Accuracy: \", eval_acc)\n",
    "    print(\"Evaluation AUC: \", eval_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e6cb3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'PySpark_CreditCard' does not exist. Creating a new experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/04 00:36:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/10/04 00:36:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9988962955825544\n",
      "Training AUC: 0.9803815205217594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy:  0.9990163358510451\n",
      "Evaluation AUC:  0.988471855115027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "\n",
    "mlflow.set_experiment(\"PySpark_CreditCard\") \n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    trainedLR = train(lr, train_set)\n",
    "    evaluate(trainedLR, test_set)\n",
    "    mlflow.spark.log_model(trainedLR, \"creditcard_model_pyspark\")\n",
    "    \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1299e0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/10/04 00:37:08 INFO mlflow.spark: 'runs:/d2a6201c6d3d474cbec9828d4fda0dea/creditcard_model_pyspark' resolved as 'file:///Users/dongzhang/Classes/mlflow/Kaggle/mlruns/6/d2a6201c6d3d474cbec9828d4fda0dea/artifacts/creditcard_model_pyspark'\n",
      "2021/10/04 00:37:08 INFO mlflow.spark: File 'file:///Users/dongzhang/Classes/mlflow/Kaggle/mlruns/6/d2a6201c6d3d474cbec9828d4fda0dea/artifacts/creditcard_model_pyspark/sparkml' is already on DFS, copy is not necessary.\n"
     ]
    }
   ],
   "source": [
    "model = mlflow.spark.load_model(\"runs:/d2a6201c6d3d474cbec9828d4fda0dea/creditcard_model_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3e3d5cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_set)\n",
    "y_true = predictions.select(['label']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17a57bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 89.429%\n",
      "Accuracy Score: 99.902%\n"
     ]
    }
   ],
   "source": [
    "print(f\"AUC Score: {roc_auc_score(y_true, y_pred):.3%}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_true, y_pred):.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ec5a3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 15.0, 'Predicted')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "ax = sns.heatmap(conf_matrix, annot=True,fmt='g') \n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44f221",
   "metadata": {},
   "source": [
    "# Local Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "add3b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying the Model\n",
    "\n",
    "import pandas as pd \n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "import numpy as np \n",
    "import subprocess\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9bf15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
